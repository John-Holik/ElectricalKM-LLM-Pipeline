{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b073c97c",
   "metadata": {},
   "source": [
    "## File Set-up\n",
    "- Dynamically determines the repository root by checking if the current directory is named \"notebooks\"\n",
    "- Adds the repository root to `sys.path` to enable imports from the `src` package\n",
    "- Imports shared project paths (`PROJECT_ROOT`, `RAW_DIR`, `PROCESSED_DIR`, `REFERENCE_DIR`, `FIGURES_DIR`) from `src.paths`\n",
    "- Defines notebook-specific file paths for input (Excel file) and outputs (CSV, JSON files)\n",
    "- Prints the resolved project root, raw data directory, and processed data directory for verification\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`nb_dir`**: Resolved current working directory\n",
    "- **`repo_root`**: Repository root (parent of `notebooks/` if applicable, otherwise `nb_dir`)\n",
    "- **`INPUT_EXCEL`**: `data/raw/electrical_construction_videos_combined_Sep19.xlsx`\n",
    "- **`VIDEO_URLS_CSV`**: `data/raw/electrical_construction_videos_max.csv`\n",
    "- **`TRANSCRIPTS_FILE`**: `data/raw/transcripts.json`\n",
    "- **`NEW_TRANSCRIPTS_FILE`**: `data/raw/new_transcripts.json`\n",
    "- **`COMMENTS_FILE`**: `data/raw/youtube_comments.json`\n",
    "\n",
    "## Inputs\n",
    "- **`src.paths` module**: Must exist in the repository and export `PROJECT_ROOT`, `RAW_DIR`, `PROCESSED_DIR`, `REFERENCE_DIR`, `FIGURES_DIR`\n",
    "- No external files are read in this cell\n",
    "\n",
    "## Outputs\n",
    "- No files are written; this cell only sets up paths and prints diagnostic messages\n",
    "- Prints: project root, raw data directory, and processed data directory paths\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes the notebook is run from either the repository root or a `notebooks/` subdirectory\n",
    "- Requires `src/paths.py` to exist and define the expected path constants\n",
    "- All subsequent cells depend on the path variables defined here (`INPUT_EXCEL`, `VIDEO_URLS_CSV`, `TRANSCRIPTS_FILE`, `NEW_TRANSCRIPTS_FILE`, `COMMENTS_FILE`)\n",
    "- The `src` package must be importable after `sys.path` modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: Ensure repo root is on sys.path for imports ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path.cwd().resolve()\n",
    "repo_root = nb_dir.parent if nb_dir.name == \"notebooks\" else nb_dir\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# === Import shared paths ===\n",
    "from src.paths import PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR\n",
    "\n",
    "# === Notebook-specific file paths ===\n",
    "# Input files (expected in data/raw/)\n",
    "INPUT_EXCEL = RAW_DIR / \"electrical_construction_videos_combined_Sep19.xlsx\"\n",
    "\n",
    "# Output files (written to data/raw/ or data/processed/)\n",
    "VIDEO_URLS_CSV = RAW_DIR / \"electrical_construction_videos_max.csv\"\n",
    "TRANSCRIPTS_FILE = RAW_DIR / \"transcripts.json\"\n",
    "NEW_TRANSCRIPTS_FILE = RAW_DIR / \"new_transcripts.json\"\n",
    "COMMENTS_FILE = RAW_DIR / \"youtube_comments.json\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data dir: {RAW_DIR}\")\n",
    "print(f\"Processed dir: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54d9a1",
   "metadata": {},
   "source": [
    "## 01_Get_URLS\n",
    "\n",
    "- Initializes the YouTube Data API v3 client using a hardcoded API key\n",
    "- Defines 15 search keywords related to electrical construction topics\n",
    "- For each keyword, searches YouTube videos (up to 500 per keyword) and filters results where the title contains \"electrical\" or \"construction\"\n",
    "- Collects video titles and URLs, respecting API rate limits with 1-second delays between requests\n",
    "- Stops early if approaching the daily quota limit (9,900 videos)\n",
    "- Removes duplicate videos from the collected results\n",
    "- Writes all unique video titles and URLs to a CSV file\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`API_KEY`**: Hardcoded YouTube Data API v3 key (`AIzaSyDCAB67SEgb4qUryc2mWKJEsOVmLpKscUg`)\n",
    "- **`API_SERVICE_NAME`**: `'youtube'`\n",
    "- **`API_VERSION`**: `'v3'`\n",
    "- **`keywords`**: List of 15 electrical construction search terms\n",
    "- **`max_results`**: 500 videos per keyword\n",
    "- **`VIDEO_URLS_CSV`**: Output path (`data/raw/electrical_construction_videos_max.csv`)\n",
    "\n",
    "## Inputs\n",
    "- **YouTube Data API v3**: Used to search for videos matching each keyword\n",
    "- No input files are read\n",
    "\n",
    "## Outputs\n",
    "- **`data/raw/electrical_construction_videos_max.csv`**: CSV file containing two columns (`Title`, `URL`) with all unique videos found across all keywords\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes the API key has sufficient quota to handle up to ~7,500 video searches (15 keywords × 500 videos)\n",
    "- Filters videos by checking if \"electrical\" or \"construction\" appears in the title (case-insensitive)\n",
    "- Uses `VIDEO_URLS_CSV` variable defined in CELL INDEX 1\n",
    "- Requires `googleapiclient` library installed\n",
    "- API quota limit is assumed to be around 10,000 units per day (stops at 9,900 videos as safety buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import time\n",
    "\n",
    "# Replace with your API key\n",
    "API_KEY = 'AIzaSyDCAB67SEgb4qUryc2mWKJEsOVmLpKscUg'\n",
    "API_SERVICE_NAME = 'youtube'\n",
    "API_VERSION = 'v3'\n",
    "\n",
    "# Initialize the YouTube API client\n",
    "youtube = build(API_SERVICE_NAME, API_VERSION, developerKey=API_KEY)\n",
    "\n",
    "# Define the search keywords\n",
    "keywords = [\n",
    "    \"Electrical construction techniques\",\n",
    "    \"Electrical wiring installation\",\n",
    "    \"Commercial electrical construction\",\n",
    "    \"Residential electrical construction\",\n",
    "    \"Industrial electrical construction\",\n",
    "    \"Electrical safety in construction\",\n",
    "    \"Electrical code compliance\",\n",
    "    \"Electrical system design\",\n",
    "    \"Electrical troubleshooting in construction\",\n",
    "    \"Electrical conduit installation\",\n",
    "    \"Electrical panel installation\",\n",
    "    \"Electrical grounding techniques\",\n",
    "    \"Electrical load calculation\",\n",
    "    \"Electrical blueprint reading\",\n",
    "    \"Electrical construction tools and equipment\"\n",
    "]\n",
    "\n",
    "def search_videos_by_keyword(keyword, max_results=500):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.search().list(\n",
    "                q=keyword,\n",
    "                part='snippet',\n",
    "                type='video',\n",
    "                maxResults=50,  # API maximum per request\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                video_title = item['snippet']['title']\n",
    "                if 'electrical' in video_title.lower() or 'construction' in video_title.lower():\n",
    "                    video_url = f\"https://www.youtube.com/watch?v={item['id']['videoId']}\"\n",
    "                    videos.append((video_title, video_url))\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token or len(videos) >= max_results:\n",
    "                break\n",
    "            \n",
    "            # Respect YouTube API rate limits\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except HttpError as e:\n",
    "            print(f\"An HTTP error {e.resp.status} occurred: {e.content}\")\n",
    "            break\n",
    "    \n",
    "    return videos[:max_results]\n",
    "\n",
    "# Collect all video titles and URLs for each keyword\n",
    "all_videos = []\n",
    "for keyword in keywords:\n",
    "    print(f\"Searching for '{keyword}'...\")\n",
    "    videos = search_videos_by_keyword(keyword)\n",
    "    all_videos.extend(videos)\n",
    "    print(f\"Found {len(videos)} videos for '{keyword}'\")\n",
    "    \n",
    "    # Check if we're approaching the daily quota limit\n",
    "    if len(all_videos) >= 9900:  # Leave some buffer for safety\n",
    "        print(\"Approaching daily quota limit. Stopping search.\")\n",
    "        break\n",
    "\n",
    "# Remove duplicates\n",
    "all_videos = list(set(all_videos))\n",
    "\n",
    "# Save the results to a CSV file (using portable path)\n",
    "with open(VIDEO_URLS_CSV, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Title', 'URL']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for title, url in all_videos:\n",
    "        writer.writerow({'Title': title, 'URL': url})\n",
    "\n",
    "print(f\"CSV file '{VIDEO_URLS_CSV}' created successfully with {len(all_videos)} unique videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382811c7",
   "metadata": {},
   "source": [
    "## 02_Get_Transcripts\n",
    "\n",
    "- Loads YouTube API key from `.env` file and initializes the YouTube Data API v3 client\n",
    "- Reads video URLs from the Excel file (`data/raw/electrical_construction_videos_combined_Sep19.xlsx`)\n",
    "- For each video, extracts the video ID, fetches video statistics (views, likes, comment count), and retrieves English transcripts\n",
    "- Handles both old and new versions of the `youtube-transcript-api` library for compatibility\n",
    "- Skips videos already processed (checks both `data/raw/new_transcripts.json` and `data/raw/transcripts.json`)\n",
    "- Saves progress incrementally after each video to `data/raw/new_transcripts.json` with atomic writes\n",
    "- Implements graceful shutdown on Ctrl+C, random delays (2-6 seconds) between requests to avoid rate limiting, and resume capability from saved checkpoints\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`API_KEY`**: YouTube Data API v3 key loaded from `.env` file via `YOUTUBE_API_KEY` environment variable\n",
    "- **`OUTPUT_FILE`**: `NEW_TRANSCRIPTS_FILE` = `data/raw/new_transcripts.json`\n",
    "- **`INPUT_EXCEL`**: `data/raw/electrical_construction_videos_combined_Sep19.xlsx` (from CELL INDEX 1)\n",
    "- **`TRANSCRIPTS_FILE`**: `data/raw/transcripts.json` (used to check previously processed videos)\n",
    "- **`youtube`**: YouTube API v3 client instance\n",
    "\n",
    "## Inputs\n",
    "- **`data/raw/electrical_construction_videos_combined_Sep19.xlsx`**: Excel file containing video URLs and titles\n",
    "- **`data/raw/new_transcripts.json`**: Existing progress file (if resuming)\n",
    "- **`data/raw/transcripts.json`**: Previous transcript data to avoid re-processing\n",
    "- **YouTube Data API v3**: Fetches video statistics\n",
    "- **`youtube-transcript-api`**: Fetches video transcripts in English\n",
    "\n",
    "## Outputs\n",
    "- **`data/raw/new_transcripts.json`**: JSON file containing array of video records with fields: `VideoID`, `Title`, `URL`, `Views`, `Likes`, `CommentCount`, `Transcript`\n",
    "- **`data/raw/new_transcripts.json.tmp`**: Temporary file used during atomic saves (automatically replaced)\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Requires `.env` file with `YOUTUBE_API_KEY` variable defined\n",
    "- Assumes Excel file has a column with \"url\" or \"link\" in the name (case-insensitive) and optionally a \"Title\" column\n",
    "- Expects `youtube-transcript-api` library installed (supports both <1.0 and >=1.2 versions)\n",
    "- Depends on path variables from CELL INDEX 1: `INPUT_EXCEL`, `NEW_TRANSCRIPTS_FILE`, `TRANSCRIPTS_FILE`\n",
    "- Uses signal handlers (SIGINT, SIGTERM) for graceful shutdown\n",
    "- Implements checkpoint/resume logic: skips videos already in either output file or old transcripts file\n",
    "- Random delays help avoid YouTube API rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c043d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# === Load API key ===\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"YOUTUBE_API_KEY not found in .env file\")\n",
    "\n",
    "# === Initialize YouTube API client ===\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# === Config (using portable paths from configuration cell) ===\n",
    "OUTPUT_FILE = NEW_TRANSCRIPTS_FILE\n",
    "running = True\n",
    "\n",
    "\n",
    "def handle_exit(sig, frame):\n",
    "    \"\"\"Handle Ctrl+C gracefully.\"\"\"\n",
    "    global running\n",
    "    print(\"\\n Ctrl+C detected. Finishing current video and saving progress...\")\n",
    "    running = False\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_exit)\n",
    "signal.signal(signal.SIGTERM, handle_exit)\n",
    "\n",
    "\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"Extracts the YouTube video ID from a URL.\"\"\"\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([A-Za-z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def get_video_stats(video_id: str):\n",
    "    \"\"\"Fetch video statistics such as views, likes, and comment count.\"\"\"\n",
    "    request = youtube.videos().list(part=\"statistics\", id=video_id)\n",
    "    response = request.execute()\n",
    "\n",
    "    stats = {}\n",
    "    if response.get(\"items\"):\n",
    "        s = response[\"items\"][0][\"statistics\"]\n",
    "        stats[\"Views\"] = int(s.get(\"viewCount\", 0))\n",
    "        stats[\"Likes\"] = int(s.get(\"likeCount\", 0))\n",
    "        stats[\"Dislikes\"] = \"N/A\"  # Hidden by YouTube since 2021\n",
    "        stats[\"CommentCount\"] = int(s.get(\"commentCount\", 0))\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_transcript(video_id: str):\n",
    "    \"\"\"\n",
    "    Fetch transcript safely for both old and new youtube-transcript-api versions.\n",
    "    Returns dict with full_text and segment list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if hasattr(YouTubeTranscriptApi, \"fetch\"):\n",
    "            # --- New API (>=1.2) ---\n",
    "            api = YouTubeTranscriptApi()\n",
    "            fetched = api.fetch(video_id=video_id, languages=[\"en-US\", \"en\"])\n",
    "            snippets = getattr(fetched, \"snippets\", [])\n",
    "            texts = [s.text for s in snippets]\n",
    "        else:\n",
    "            # --- Old API (<1.0) ---\n",
    "            snippets = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en-US\", \"en\"])\n",
    "            texts = [s.get(\"text\", \"\") for s in snippets]\n",
    "\n",
    "        return {\"full_text\": \" \".join(texts).strip()}\n",
    "\n",
    "    except (TranscriptsDisabled, NoTranscriptFound):\n",
    "        return {\"full_text\": \"Transcript not available\"}\n",
    "    except Exception as e:\n",
    "        return {\"full_text\": f\"Error: {e}\"}\n",
    "\n",
    "\n",
    "def load_existing_data(file_path):\n",
    "    \"\"\"Load saved progress from JSON if available.\"\"\"\n",
    "    if file_path.exists() if hasattr(file_path, 'exists') else os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"Loaded existing data ({len(data)} records) from {file_path}\")\n",
    "                return data\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Corrupt JSON file found at {file_path}. Starting fresh.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def save_progress(data, file_path):\n",
    "    \"\"\"Safely save progress to JSON.\"\"\"\n",
    "    tmp_file = str(file_path) + \".tmp\"\n",
    "    with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    os.replace(tmp_file, file_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    global running\n",
    "\n",
    "    # === Load Excel file ===\n",
    "    df = pd.read_excel(INPUT_EXCEL)\n",
    "    possible_columns = [c for c in df.columns if \"url\" in c.lower() or \"link\" in c.lower()]\n",
    "    if not possible_columns:\n",
    "        raise ValueError(\"No URL column found in Excel file.\")\n",
    "    url_col = possible_columns[0]\n",
    "\n",
    "    # === Load checkpoints ===\n",
    "    results = load_existing_data(OUTPUT_FILE)\n",
    "    transcripts = load_existing_data(TRANSCRIPTS_FILE)\n",
    "\n",
    "    processed_ids = {v[\"VideoID\"] for v in results}\n",
    "    transcript_ids = {v[\"VideoID\"] for v in transcripts}\n",
    "\n",
    "    # === Start processing videos ===\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Fetching video details\"):\n",
    "        if not running:\n",
    "            break\n",
    "\n",
    "        url = str(row[url_col])\n",
    "        title = str(row.get(\"Title\", \"Unknown Title\"))\n",
    "        video_id = extract_video_id(url)\n",
    "\n",
    "        if not video_id:\n",
    "            continue\n",
    "\n",
    "        # Skip if video already in output or transcripts\n",
    "        if video_id in processed_ids or video_id in transcript_ids:\n",
    "            print(f\" Skipping {video_id} (already processed)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            stats = get_video_stats(video_id)\n",
    "            transcript_data = get_transcript(video_id)\n",
    "\n",
    "            video_data = {\n",
    "                \"VideoID\": video_id,\n",
    "                \"Title\": title,\n",
    "                \"URL\": url,\n",
    "                \"Views\": stats.get(\"Views\"),\n",
    "                \"Likes\": stats.get(\"Likes\"),\n",
    "                \"CommentCount\": stats.get(\"CommentCount\"),\n",
    "                \"Transcript\": transcript_data[\"full_text\"]\n",
    "            }\n",
    "\n",
    "            results.append(video_data)\n",
    "            processed_ids.add(video_id)\n",
    "            save_progress(results, OUTPUT_FILE)  # Save after each video\n",
    "\n",
    "            # Random delay (2–6 seconds) to prevent rate limiting\n",
    "            sleep_time = random.uniform(2, 6)\n",
    "            print(f\" Waiting {sleep_time:.2f}s before next request...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error fetching {url}: {e}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    save_progress(results, OUTPUT_FILE)\n",
    "    print(f\"\\n Done! Saved {len(results)} video records to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed09e6",
   "metadata": {},
   "source": [
    "### 03_Remove_Record_With_No_Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for removing records with no transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c54d86",
   "metadata": {},
   "source": [
    "## 04_Get_Comments\n",
    "\n",
    "- Loads YouTube API key from `.env` file and initializes the YouTube Data API v3 client\n",
    "- Reads video URLs and titles from the Excel file (`data/raw/electrical_construction_videos_combined_Sep19.xlsx`)\n",
    "- For each video, extracts the video ID and fetches all top-level comments and their replies using the YouTube API\n",
    "- Skips videos already processed (checks `data/raw/youtube_comments.json` for previously saved video IDs)\n",
    "- Saves progress incrementally after each video to `data/raw/youtube_comments.json` with atomic writes\n",
    "- Implements graceful shutdown on Ctrl+C, limiting comment pages to 50 per video, and resume capability from saved checkpoints\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`API_KEY`**: YouTube Data API v3 key loaded from `.env` file via `YOUTUBE_API_KEY` environment variable\n",
    "- **`OUTPUT_FILE`**: `COMMENTS_FILE` = `data/raw/youtube_comments.json`\n",
    "- **`INPUT_EXCEL`**: `data/raw/electrical_construction_videos_combined_Sep19.xlsx` (from CELL INDEX 1)\n",
    "- **`SAVE_INTERVAL`**: `1` (saves after every video processed)\n",
    "- **`max_pages`**: `50` (maximum comment pages to fetch per video)\n",
    "- **`youtube`**: YouTube API v3 client instance\n",
    "\n",
    "## Inputs\n",
    "- **`data/raw/electrical_construction_videos_combined_Sep19.xlsx`**: Excel file containing video URLs and titles\n",
    "- **`data/raw/youtube_comments.json`**: Existing progress file (if resuming)\n",
    "- **YouTube Data API v3**: Fetches comment threads (top-level comments and replies) for each video\n",
    "- **`.env` file**: Must contain `YOUTUBE_API_KEY` variable\n",
    "\n",
    "## Outputs\n",
    "- **`data/raw/youtube_comments.json`**: JSON file containing array of video records with fields: `VideoID`, `Title`, `URL`, `Comments` (array of comment objects with `CommentID`, `Author`, `Text`, `Likes`, `PublishedAt`, `ParentID`, `Replies`)\n",
    "- **`data/raw/youtube_comments.json.tmp`**: Temporary file used during atomic saves (automatically replaced)\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Requires `.env` file with `YOUTUBE_API_KEY` variable defined\n",
    "- Assumes Excel file has a column with \"url\" or \"link\" in the name (case-insensitive) and optionally a \"Title\" column\n",
    "- Depends on path variables from CELL INDEX 1: `INPUT_EXCEL`, `COMMENTS_FILE`\n",
    "- Uses signal handlers (SIGINT, SIGTERM) for graceful shutdown\n",
    "- Implements checkpoint/resume logic: skips videos already in the output file\n",
    "- Each comment object includes nested `Replies` array for threaded discussions\n",
    "- Comment fetching is limited to 100 comments per API request and up to 50 pages per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a212436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"YouTube API key not found in .env file.\")\n",
    "\n",
    "# Initialize YouTube client\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Globals (using portable paths from configuration cell)\n",
    "OUTPUT_FILE = COMMENTS_FILE\n",
    "SAVE_INTERVAL = 1  # Save after every video\n",
    "running = True\n",
    "\n",
    "\n",
    "def handle_exit(sig, frame):\n",
    "    \"\"\"Handle Ctrl+C gracefully.\"\"\"\n",
    "    global running\n",
    "    print(\"\\n Ctrl+C pressed. Finishing current video and saving progress...\")\n",
    "    running = False\n",
    "\n",
    "\n",
    "# Register signal handler\n",
    "signal.signal(signal.SIGINT, handle_exit)\n",
    "signal.signal(signal.SIGTERM, handle_exit)\n",
    "\n",
    "\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"Extract the video ID from a YouTube URL.\"\"\"\n",
    "    match = re.search(r\"(?:v=|youtu\\.be/)([A-Za-z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def get_all_comments(video_id: str, title: str, url: str, max_pages=50):\n",
    "    \"\"\"Fetch all comments (and replies) for a video.\"\"\"\n",
    "    comments_data = []\n",
    "    next_page_token = None\n",
    "    page_count = 0\n",
    "\n",
    "    while True:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet,replies\",\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            top = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            top_comment = {\n",
    "                \"CommentID\": item[\"id\"],\n",
    "                \"Author\": top.get(\"authorDisplayName\"),\n",
    "                \"Text\": top.get(\"textDisplay\"),\n",
    "                \"Likes\": top.get(\"likeCount\", 0),\n",
    "                \"PublishedAt\": top.get(\"publishedAt\"),\n",
    "                \"ParentID\": None,\n",
    "                \"Replies\": []\n",
    "            }\n",
    "\n",
    "            if \"replies\" in item:\n",
    "                for reply in item[\"replies\"][\"comments\"]:\n",
    "                    reply_snip = reply[\"snippet\"]\n",
    "                    reply_comment = {\n",
    "                        \"CommentID\": reply[\"id\"],\n",
    "                        \"Author\": reply_snip.get(\"authorDisplayName\"),\n",
    "                        \"Text\": reply_snip.get(\"textDisplay\"),\n",
    "                        \"Likes\": reply_snip.get(\"likeCount\", 0),\n",
    "                        \"PublishedAt\": reply_snip.get(\"publishedAt\"),\n",
    "                        \"ParentID\": item[\"id\"]\n",
    "                    }\n",
    "                    top_comment[\"Replies\"].append(reply_comment)\n",
    "\n",
    "            comments_data.append(top_comment)\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        page_count += 1\n",
    "        if not next_page_token or page_count >= max_pages or not running:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"VideoID\": video_id,\n",
    "        \"Title\": title,\n",
    "        \"URL\": url,\n",
    "        \"Comments\": comments_data\n",
    "    }\n",
    "\n",
    "\n",
    "def load_existing_data():\n",
    "    \"\"\"Load existing JSON progress (if available).\"\"\"\n",
    "    if OUTPUT_FILE.exists() if hasattr(OUTPUT_FILE, 'exists') else os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                print(f\" Resuming from previous progress: {len(data)} videos already saved.\")\n",
    "                return data\n",
    "            except json.JSONDecodeError:\n",
    "                print(\" Warning: JSON file corrupt or empty. Starting fresh.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def save_progress(data):\n",
    "    \"\"\"Save JSON data safely to disk.\"\"\"\n",
    "    tmp_file = str(OUTPUT_FILE) + \".tmp\"\n",
    "    with open(tmp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    os.replace(tmp_file, OUTPUT_FILE)\n",
    "\n",
    "\n",
    "def main():\n",
    "    global running\n",
    "\n",
    "    # Load Excel file\n",
    "    df = pd.read_excel(INPUT_EXCEL)\n",
    "\n",
    "    # Detect URL column\n",
    "    possible_columns = [c for c in df.columns if \"url\" in c.lower() or \"link\" in c.lower()]\n",
    "    if not possible_columns:\n",
    "        raise ValueError(\"No column with YouTube URLs found. Please include a 'URL' column.\")\n",
    "    url_col = possible_columns[0]\n",
    "\n",
    "    # Load existing data\n",
    "    results = load_existing_data()\n",
    "    processed_ids = {v[\"VideoID\"] for v in results}\n",
    "\n",
    "    # Iterate over new videos\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Fetching comments\"):\n",
    "        if not running:\n",
    "            break\n",
    "\n",
    "        url = str(row[url_col])\n",
    "        title = str(row.get(\"Title\", \"Unknown Title\"))\n",
    "        video_id = extract_video_id(url)\n",
    "        if not video_id:\n",
    "            print(f\" Skipping invalid URL: {url}\")\n",
    "            continue\n",
    "\n",
    "        if video_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\" Processing video: {video_id} - {title[:50]}...\")\n",
    "            video_data = get_all_comments(video_id, title, url)\n",
    "            results.append(video_data)\n",
    "            processed_ids.add(video_id)\n",
    "\n",
    "            # Save after every video\n",
    "            if len(results) % SAVE_INTERVAL == 0:\n",
    "                save_progress(results)\n",
    "\n",
    "            if not running:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error fetching {url}: {e}\")\n",
    "            time.sleep(2)  # minor delay to avoid rate-limit issues\n",
    "\n",
    "    # Final save\n",
    "    save_progress(results)\n",
    "    print(f\"\\n Done! Saved {len(results)} videos to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9ecde",
   "metadata": {},
   "source": [
    "### 05_Remove_Videos_With_No_Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to remove records with no comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f6f26",
   "metadata": {},
   "source": [
    "### 06_Data_Collection_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to show the collection results, such as total # of videos and comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
