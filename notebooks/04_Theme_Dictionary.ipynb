{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccb52e0",
   "metadata": {},
   "source": [
    "## File Set-Up\n",
    "- Adds the repository root directory to `sys.path` to enable imports from the `src` package\n",
    "- Determines the repository root by checking if the current directory is named \"notebooks\" (if so, uses parent; otherwise uses current directory)\n",
    "- Imports standard path constants from `src.paths` module (PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR)\n",
    "- Defines notebook-specific input file paths for transcript summaries and comment analysis CSVs\n",
    "- Defines notebook-specific output file paths for theme dictionary JSON, checkpoint JSON, and thematic analysis CSV\n",
    "- Prints confirmation messages showing resolved paths for verification\n",
    "\n",
    "## Configuration / Setup\n",
    "- **Repository structure assumption**: notebook is either in a `notebooks/` subdirectory or at the repository root\n",
    "- **Path constants** (imported from `src.paths`):\n",
    "    - `PROJECT_ROOT`: repository root directory\n",
    "    - `RAW_DIR`: raw data directory\n",
    "    - `PROCESSED_DIR`: processed data directory\n",
    "    - `REFERENCE_DIR`: reference data directory\n",
    "    - `FIGURES_DIR`: figures output directory\n",
    "\n",
    "## Inputs\n",
    "- **`PROCESSED_DIR / \"GPT_5_Mini_Transcripts_SummaryV2.csv\"`**: transcript summary data (input)\n",
    "- **`PROCESSED_DIR / \"GPT_5_Mini_Comment_Analysis.csv\"`**: comment analysis data (input)\n",
    "- **`src.paths` module**: must exist at repository root to provide path constants\n",
    "\n",
    "## Outputs\n",
    "- **`REFERENCE_DIR / \"Theme_Dictionary.json\"`**: dynamic theme dictionary (created if missing, grows during processing)\n",
    "- **`PROCESSED_DIR / \"Theme_Checkpoint.json\"`**: target path for processing checkpoint (defined but not created in this cell)\n",
    "- **`PROCESSED_DIR / \"Thematic_Analysis_Output.csv\"`**: target path for thematic analysis results (defined but not created in this cell)\n",
    "- **Console output**: prints paths for verification\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Requires a `src/` package at the repository root with a `paths.py` module exporting the standard directory constants\n",
    "- Path resolution logic assumes either: (1) notebook is in `<repo>/notebooks/` or (2) notebook is at `<repo>/`\n",
    "- This is a pure configuration cell; it performs no data processing or file I/O beyond path setup\n",
    "- All subsequent cells in the notebook depend on these path variable definitions\n",
    "- **Theme Dictionary Behavior**: The dictionary is dynamically managed—created empty if missing, or loaded and extended during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: Ensure repo root is on sys.path for imports ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path.cwd().resolve()\n",
    "repo_root = nb_dir.parent if nb_dir.name == \"notebooks\" else nb_dir\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# === Import shared paths ===\n",
    "from src.paths import PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR\n",
    "\n",
    "# === Notebook-specific file paths ===\n",
    "# Input files\n",
    "TRANSCRIPT_SUMMARY_CSV = PROCESSED_DIR / \"GPT_5_Mini_Transcripts_SummaryV2.csv\"\n",
    "COMMENT_ANALYSIS_CSV = PROCESSED_DIR / \"GPT_5_Mini_Comment_Analysis.csv\"\n",
    "\n",
    "# Output files\n",
    "THEME_DICT_PATH = REFERENCE_DIR / \"Theme_Dictionary.json\"\n",
    "CHECKPOINT_PATH = PROCESSED_DIR / \"Theme_Checkpoint.json\"\n",
    "THEMATIC_OUTPUT_CSV = PROCESSED_DIR / \"Thematic_Analysis_Output.csv\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Transcript summary input: {TRANSCRIPT_SUMMARY_CSV}\")\n",
    "print(f\"Theme dictionary output: {THEME_DICT_PATH}\")\n",
    "print(f\"Thematic analysis output: {THEMATIC_OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5add1e9",
   "metadata": {},
   "source": [
    "## 01_Create Theme Dictionary for Comments using Transcript Summary\n",
    "- Loads environment variables and initializes OpenAI client with `gpt-5-mini` model\n",
    "- Reads transcript summaries and comment analysis CSVs from `PROCESSED_DIR`\n",
    "- **Dynamically manages theme dictionary**: creates new file if missing, or loads existing themes to reuse\n",
    "- Groups comments by VideoID and processes them in dynamically-sized batches (5-25 comments depending on theme count)\n",
    "- For each batch, constructs a prompt instructing the model to:\n",
    "  - **Reuse existing themes** if the model has ≥65% certainty the theme applies\n",
    "  - **Create new themes** only when no existing theme fits with sufficient confidence\n",
    "  - Avoid being too specific when creating themes to encourage reuse\n",
    "  - Target staying under 100 total themes by preferring reuse\n",
    "- Sends batch to OpenAI API with retry logic (up to 6 attempts with exponential backoff for connection/timeout errors)\n",
    "- Marks comments as processed in checkpoint **before** API call to prevent re-processing on failure\n",
    "- Parses JSON response containing theme classifications and any new themes created\n",
    "- **Updates theme dictionary** with any new themes returned by the model\n",
    "- Appends classification results to output CSV with columns: VideoID, CommentID, ThemesJSON\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`OPENAI_API_KEY`**: must be set in `.env` file for API access\n",
    "- **`MODEL_NAME`**: set to `\"gpt-5-mini\"`\n",
    "- **`BATCH_SIZE`**: base default of 20, dynamically adjusted based on theme count (5 for >60 themes, 25 for <40 themes)\n",
    "- **`THEME_REUSE_THRESHOLD`**: 65% certainty required to reuse an existing theme\n",
    "- **`MAX_THEMES_TARGET`**: soft limit of 100 themes to encourage reuse\n",
    "- **Token estimation limit**: batches are dynamically resized to keep prompt under ~6000 tokens\n",
    "- **Retry settings**: max 6 retries with 4-second base backoff multiplier\n",
    "\n",
    "## Inputs\n",
    "- **`PROCESSED_DIR / \"GPT_5_Mini_Transcripts_SummaryV2.csv\"`**: contains VideoID and Summary columns for video context\n",
    "- **`PROCESSED_DIR / \"GPT_5_Mini_Comment_Analysis.csv\"`**: contains VideoID, CommentID, CommentIndex, and Topic columns for comments to classify\n",
    "- **`REFERENCE_DIR / \"Theme_Dictionary.json\"`**: dynamic theme dictionary (created if missing, updated with new themes)\n",
    "- **`PROCESSED_DIR / \"Theme_Checkpoint.json\"`**: tracks which comments have been processed (loaded if exists)\n",
    "- **OpenAI Chat Completions API**: called with JSON response format for each batch\n",
    "\n",
    "## Outputs\n",
    "- **`PROCESSED_DIR / \"Thematic_Analysis_Output.csv\"`**: CSV with columns VideoID, CommentID, ThemesJSON (where ThemesJSON is a JSON string mapping theme names to percentages)\n",
    "- **`PROCESSED_DIR / \"Theme_Checkpoint.json\"`**: updated after each batch with processed comment keys (`\"VideoID::CommentIndex\"`)\n",
    "- **`REFERENCE_DIR / \"Theme_Dictionary.json\"`**: dynamically updated with new themes as they are created during classification\n",
    "- **Console output**: progress messages, batch token estimates, new themes created, warnings for percentage mismatches\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Requires `OPENAI_API_KEY` in environment and `.env` file loaded via `python-dotenv`\n",
    "- Assumes `COMMENT_ANALYSIS_CSV` has columns: `VideoID`, `CommentID`, `CommentIndex`, `Topic` (singular, not plural)\n",
    "- Assumes `TRANSCRIPT_SUMMARY_CSV` has columns: `VideoID`, `Summary`\n",
    "- Theme dictionary structure: `{\"themes\": [{\"name\": \"...\", \"description\": \"...\"}, ...], \"count\": int}`\n",
    "- **Theme reuse logic**: Model must have ≥65% certainty to use an existing theme; otherwise creates a new one\n",
    "- **Theme creation guidance**: New themes should be broad enough for reuse, not overly specific\n",
    "- Checkpoint behavior: comments are marked processed **before** API call to prevent duplicate calls on retry/crash\n",
    "- Video summary is truncated to first 1000 characters to reduce token usage\n",
    "- Batch size dynamically halves if estimated tokens exceed 6000 (minimum batch size = 1)\n",
    "- Expects model to return JSON with `\"classifications\"` array and optional `\"new_themes\"` array\n",
    "- All file I/O uses `pathlib.Path` objects defined in setup cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f72aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import APIConnectionError, APITimeoutError\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG (using portable paths from configuration cell)\n",
    "# =========================\n",
    "\n",
    "MODEL_NAME = \"gpt-5-mini\"\n",
    "BATCH_SIZE = 20  # base default; dynamic per theme count\n",
    "THEME_REUSE_THRESHOLD = 65  # minimum certainty % to reuse an existing theme\n",
    "MAX_THEMES_TARGET = 100  # soft limit to encourage theme reuse\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UTILITIES\n",
    "# =========================\n",
    "\n",
    "def load_theme_dictionary(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load existing theme dictionary or create an empty one if it doesn't exist.\n",
    "    Returns dict with 'themes' list and 'count' integer.\n",
    "    \"\"\"\n",
    "    if path.exists():\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            # Ensure count is present\n",
    "            if \"count\" not in data:\n",
    "                data[\"count\"] = len(data.get(\"themes\", []))\n",
    "            print(f\"Loaded existing theme dictionary with {data['count']} themes\")\n",
    "            return data\n",
    "    \n",
    "    # Create new empty theme dictionary\n",
    "    theme_dict = {\"themes\": [], \"count\": 0}\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(theme_dict, f, indent=2)\n",
    "    print(\"Created new empty theme dictionary\")\n",
    "    return theme_dict\n",
    "\n",
    "\n",
    "def save_theme_dictionary(path: Path, theme_dict: Dict[str, Any]) -> None:\n",
    "    \"\"\"Save the theme dictionary to disk.\"\"\"\n",
    "    theme_dict[\"count\"] = len(theme_dict.get(\"themes\", []))\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(theme_dict, f, indent=2)\n",
    "\n",
    "\n",
    "def add_new_themes(theme_dict: Dict[str, Any], new_themes: List[Dict[str, str]], path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Add new themes to the dictionary if they don't already exist.\n",
    "    Saves to disk after adding.\n",
    "    \"\"\"\n",
    "    existing_names = {t[\"name\"].lower() for t in theme_dict.get(\"themes\", [])}\n",
    "    added_count = 0\n",
    "    \n",
    "    for theme in new_themes:\n",
    "        theme_name = theme.get(\"name\", \"\").strip()\n",
    "        if not theme_name:\n",
    "            continue\n",
    "        if theme_name.lower() not in existing_names:\n",
    "            theme_dict[\"themes\"].append({\n",
    "                \"name\": theme_name,\n",
    "                \"description\": theme.get(\"description\", \"\")\n",
    "            })\n",
    "            existing_names.add(theme_name.lower())\n",
    "            added_count += 1\n",
    "            print(f\"  + New theme added: '{theme_name}'\")\n",
    "    \n",
    "    if added_count > 0:\n",
    "        save_theme_dictionary(path, theme_dict)\n",
    "        print(f\"  Total themes now: {len(theme_dict['themes'])}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(path: Path) -> Set[str]:\n",
    "    if not path.exists():\n",
    "        return set()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return set(data.get(\"processed_comment_keys\", []))\n",
    "\n",
    "\n",
    "def save_checkpoint(path: Path, processed_keys: Set[str]) -> None:\n",
    "    data = {\"processed_comment_keys\": sorted(processed_keys)}\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def init_output_csv(path: Path) -> None:\n",
    "    \"\"\"Output only VideoID, CommentID, ThemeName→% JSON\"\"\"\n",
    "    if path.exists():\n",
    "        return\n",
    "    with path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"VideoID\", \"CommentID\", \"ThemesJSON\"])\n",
    "\n",
    "\n",
    "def append_classifications_to_csv(\n",
    "    path: Path,\n",
    "    classifications: List[Dict[str, Any]],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes rows:\n",
    "    - VideoID\n",
    "    - CommentID\n",
    "    - ThemesJSON (mapping theme_name → percentage)\n",
    "    \"\"\"\n",
    "    with path.open(\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for c in classifications:\n",
    "            theme_map = {}\n",
    "            for t in c.get(\"themes\", []):\n",
    "                theme_name = t[\"theme\"]\n",
    "                pct = t[\"percentage\"]\n",
    "                theme_map[theme_name] = pct\n",
    "\n",
    "            writer.writerow([\n",
    "                c.get(\"video_id\"),\n",
    "                c.get(\"comment_id\"),\n",
    "                json.dumps(theme_map)\n",
    "            ])\n",
    "\n",
    "        # Force flush to disk so long runs don't lose buffered rows\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TOKEN APPROX + MESSAGE BUILD\n",
    "# =========================\n",
    "\n",
    "def approx_tokens(s: str) -> int:\n",
    "    # Rough ~4 chars per token rule of thumb\n",
    "    return int(len(s) / 4)\n",
    "\n",
    "\n",
    "def build_messages_for_batch(\n",
    "    theme_payload: Dict[str, Any],\n",
    "    video_id: Any,\n",
    "    summary: str,\n",
    "    comments_payload: List[Dict[str, Any]],\n",
    "    theme_reuse_threshold: int,\n",
    "    max_themes_target: int\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build messages for dynamic theme classification.\n",
    "    theme_payload: {\"themes\": [{\"name\": \"...\", \"description\": \"...\"}, ...], \"count\": int}\n",
    "    comments_payload: list of dicts with \"comment_key\", \"comment_index\", \"comment_id\", \"topics\"\n",
    "    \"\"\"\n",
    "\n",
    "    current_theme_count = theme_payload.get(\"count\", 0)\n",
    "    themes_remaining = max(0, max_themes_target - current_theme_count)\n",
    "\n",
    "    system_instructions = f\"\"\"You are an electrical engineering expert performing thematic classification of YouTube comments.\n",
    "\n",
    "THEME MANAGEMENT RULES:\n",
    "1. You are given the current theme dictionary with existing themes (may be empty initially).\n",
    "2. For each comment, determine which theme(s) apply.\n",
    "3. REUSE an existing theme if you have at least {theme_reuse_threshold}% certainty it applies to the comment.\n",
    "4. CREATE a new theme ONLY when no existing theme fits with {theme_reuse_threshold}%+ certainty.\n",
    "5. When creating new themes:\n",
    "   - Make them BROAD enough to be reused for similar future comments\n",
    "   - Avoid overly specific themes that only apply to one comment\n",
    "   - Use clear, descriptive names related to electrical engineering/construction\n",
    "   - Include a brief description of what the theme covers\n",
    "6. Current theme count: {current_theme_count}. Target maximum: {max_themes_target}. \n",
    "   Approximately {themes_remaining} new themes can be created before reaching the soft limit.\n",
    "   STRONGLY prefer reusing existing themes when possible.\n",
    "\n",
    "CLASSIFICATION RULES:\n",
    "1. Each comment may map to one or more themes (existing or newly created).\n",
    "2. For each comment, assign INTEGER percentages to themes that sum to EXACTLY 100.\n",
    "3. Be consistent: similar comments should be classified to the same themes.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON):\n",
    "{{\n",
    "  \"new_themes\": [\n",
    "    {{\"name\": \"Theme Name Here\", \"description\": \"Brief description of what this theme covers\"}}\n",
    "  ],\n",
    "  \"classifications\": [\n",
    "    {{\n",
    "      \"video_id\": \"...\",\n",
    "      \"comment_key\": \"...\",\n",
    "      \"comment_index\": <int>,\n",
    "      \"comment_id\": \"... or null\",\n",
    "      \"themes\": [\n",
    "        {{\"theme\": \"Existing or New Theme Name\", \"percentage\": 60}},\n",
    "        {{\"theme\": \"Another Theme\", \"percentage\": 40}}\n",
    "      ]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- \"new_themes\" array contains ONLY themes you are creating (not existing ones)\n",
    "- \"new_themes\" can be empty [] if all comments fit existing themes\n",
    "- Theme names in \"classifications\" must match EXACTLY (existing theme names or new theme names)\n",
    "- No extra keys. No explanations. Strict JSON only.\"\"\"\n",
    "\n",
    "    # Build theme list for the prompt\n",
    "    if theme_payload.get(\"themes\"):\n",
    "        theme_list_str = json.dumps(theme_payload[\"themes\"], ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        theme_list_str = \"[] (No themes yet - you will create the initial themes)\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_instructions},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"CURRENT THEME DICTIONARY ({current_theme_count} themes):\\n{theme_list_str}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"VideoID: {video_id}\\nVideo Summary (for context):\\n{summary}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Comments to classify (use existing themes when ≥65% confident, create new themes otherwise):\\n\" +\n",
    "                       json.dumps(comments_payload, ensure_ascii=False, indent=2)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# OPENAI CALL (with retries, dynamic theme handling)\n",
    "# =========================\n",
    "\n",
    "def call_openai_for_batch(\n",
    "    client: OpenAI,\n",
    "    theme_payload: Dict[str, Any],\n",
    "    video_id: Any,\n",
    "    summary: str,\n",
    "    comments_payload: List[Dict[str, Any]],\n",
    "    theme_reuse_threshold: int,\n",
    "    max_themes_target: int\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    messages = build_messages_for_batch(\n",
    "        theme_payload, video_id, summary, comments_payload,\n",
    "        theme_reuse_threshold, max_themes_target\n",
    "    )\n",
    "\n",
    "    max_retries = 6\n",
    "    backoff_base = 4\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            break\n",
    "\n",
    "        except (APIConnectionError, APITimeoutError, httpx.ReadTimeout) as e:\n",
    "            print(f\"[Attempt {attempt}/{max_retries}] Timeout/Connection Error: {e}\")\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            wait_time = backoff_base * attempt\n",
    "            print(f\"Retrying in {wait_time} seconds...\\n\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        raise RuntimeError(f\"Model returned invalid JSON:\\n{content}\")\n",
    "\n",
    "    if \"classifications\" not in data:\n",
    "        raise RuntimeError(f\"Model JSON missing 'classifications':\\n{json.dumps(data, indent=2)}\")\n",
    "\n",
    "    # Ensure new_themes exists (may be empty)\n",
    "    if \"new_themes\" not in data:\n",
    "        data[\"new_themes\"] = []\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY missing in .env\")\n",
    "\n",
    "    client = OpenAI(timeout=60)\n",
    "\n",
    "    transcripts_df = pd.read_csv(TRANSCRIPT_SUMMARY_CSV)\n",
    "    comments_df = pd.read_csv(COMMENT_ANALYSIS_CSV)\n",
    "\n",
    "    comments_df = comments_df.reset_index().rename(columns={\"index\": \"CommentIndex\"})\n",
    "\n",
    "    # Load or create theme dictionary\n",
    "    theme_dict = load_theme_dictionary(THEME_DICT_PATH)\n",
    "\n",
    "    # Dynamic batch base size depending on how many themes we have\n",
    "    theme_count = len(theme_dict.get(\"themes\", []))\n",
    "    if theme_count > 60:\n",
    "        base_batch_size = 5\n",
    "    elif theme_count > 40:\n",
    "        base_batch_size = 10\n",
    "    elif theme_count < 10:\n",
    "        # Smaller batches at start to build up themes gradually\n",
    "        base_batch_size = 10\n",
    "    else:\n",
    "        base_batch_size = BATCH_SIZE\n",
    "\n",
    "    print(f\"Starting with {theme_count} themes. Using base batch size = {base_batch_size}\")\n",
    "    print(f\"Theme reuse threshold: {THEME_REUSE_THRESHOLD}%\")\n",
    "    print(f\"Max themes target: {MAX_THEMES_TARGET}\")\n",
    "\n",
    "    processed_keys = load_checkpoint(CHECKPOINT_PATH)\n",
    "    init_output_csv(THEMATIC_OUTPUT_CSV)\n",
    "\n",
    "    video_summary_map = transcripts_df.set_index(\"VideoID\")[\"Summary\"].to_dict()\n",
    "    grouped = comments_df.groupby(\"VideoID\")\n",
    "\n",
    "    try:\n",
    "        for video_id, group in grouped:\n",
    "            if video_id not in video_summary_map:\n",
    "                continue\n",
    "\n",
    "            # Truncate summary to reduce token usage\n",
    "            raw_summary = str(video_summary_map[video_id])\n",
    "            summary = raw_summary[:1000]\n",
    "\n",
    "            group = group.sort_values(\"CommentIndex\")\n",
    "\n",
    "            rows_to_process = []\n",
    "            for _, row in group.iterrows():\n",
    "                ck = f\"{video_id}::{int(row['CommentIndex'])}\"\n",
    "                if ck not in processed_keys:\n",
    "                    rows_to_process.append(row)\n",
    "\n",
    "            if not rows_to_process:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nStarting VideoID={video_id}, {len(rows_to_process)} comments to process\")\n",
    "\n",
    "            # Pointer-based loop so we can adapt batch size dynamically per batch\n",
    "            idx = 0\n",
    "            total = len(rows_to_process)\n",
    "\n",
    "            while idx < total:\n",
    "                # Recalculate batch size based on current theme count\n",
    "                current_theme_count = len(theme_dict.get(\"themes\", []))\n",
    "                if current_theme_count > 60:\n",
    "                    cur_base_batch = 5\n",
    "                elif current_theme_count > 40:\n",
    "                    cur_base_batch = 10\n",
    "                elif current_theme_count < 10:\n",
    "                    cur_base_batch = 10\n",
    "                else:\n",
    "                    cur_base_batch = base_batch_size\n",
    "\n",
    "                cur_batch_size = min(cur_base_batch, total - idx)\n",
    "\n",
    "                while True:\n",
    "                    batch_rows = rows_to_process[idx: idx + cur_batch_size]\n",
    "\n",
    "                    # Build comments payload for this tentative batch\n",
    "                    comments_payload = []\n",
    "                    for row in batch_rows:\n",
    "                        comment_index = int(row[\"CommentIndex\"])\n",
    "                        comment_id = None\n",
    "                        if \"CommentID\" in row and not pd.isna(row[\"CommentID\"]):\n",
    "                            comment_id = str(row[\"CommentID\"])\n",
    "\n",
    "                        topics = str(row[\"Topic\"]) if \"Topic\" in row else \"\"\n",
    "                        comments_payload.append({\n",
    "                            \"comment_key\": f\"{video_id}::{comment_index}\",\n",
    "                            \"comment_index\": comment_index,\n",
    "                            \"comment_id\": comment_id,\n",
    "                            \"topics\": topics\n",
    "                        })\n",
    "\n",
    "                    # Build theme payload for API call\n",
    "                    theme_payload = {\n",
    "                        \"themes\": theme_dict.get(\"themes\", []),\n",
    "                        \"count\": len(theme_dict.get(\"themes\", []))\n",
    "                    }\n",
    "\n",
    "                    # Approximate token usage for this prompt\n",
    "                    prompt_body = (\n",
    "                        json.dumps(theme_payload, separators=(',', ':')) +\n",
    "                        summary +\n",
    "                        json.dumps(comments_payload, separators=(',', ':'))\n",
    "                    )\n",
    "                    token_est = approx_tokens(prompt_body)\n",
    "\n",
    "                    if token_est <= 6000 or cur_batch_size == 1:\n",
    "                        # Accept this batch size\n",
    "                        break\n",
    "\n",
    "                    # Too big: shrink batch size and try again\n",
    "                    cur_batch_size = max(1, cur_batch_size // 2)\n",
    "\n",
    "                # === Mark all keys in this batch as processed BEFORE the API call ===\n",
    "                batch_keys = [cp[\"comment_key\"] for cp in comments_payload]\n",
    "                processed_keys.update(batch_keys)\n",
    "                save_checkpoint(CHECKPOINT_PATH, processed_keys)\n",
    "\n",
    "                print(f\"Processing batch: idx={idx}, size={len(comments_payload)}, themes={theme_payload['count']}, tokens≈{token_est}\")\n",
    "\n",
    "                result = call_openai_for_batch(\n",
    "                    client=client,\n",
    "                    theme_payload=theme_payload,\n",
    "                    video_id=video_id,\n",
    "                    summary=summary,\n",
    "                    comments_payload=comments_payload,\n",
    "                    theme_reuse_threshold=THEME_REUSE_THRESHOLD,\n",
    "                    max_themes_target=MAX_THEMES_TARGET\n",
    "                )\n",
    "\n",
    "                # Handle new themes\n",
    "                new_themes = result.get(\"new_themes\", [])\n",
    "                if new_themes:\n",
    "                    print(f\"  Model created {len(new_themes)} new theme(s):\")\n",
    "                    add_new_themes(theme_dict, new_themes, THEME_DICT_PATH)\n",
    "\n",
    "                classifications = result[\"classifications\"]\n",
    "\n",
    "                # Normalize / sanity-check classifications\n",
    "                formatted_rows = []\n",
    "                for c in classifications:\n",
    "                    comment_key = c.get(\"comment_key\")\n",
    "                    comment_index = c.get(\"comment_index\")\n",
    "                    comment_id = c.get(\"comment_id\", None)\n",
    "                    themes = c.get(\"themes\", [])\n",
    "\n",
    "                    total_pct = sum(t.get(\"percentage\", 0) for t in themes)\n",
    "                    if total_pct != 100:\n",
    "                        print(f\"  WARNING: Percentages for comment_key={comment_key} sum to {total_pct}, not 100.\")\n",
    "\n",
    "                    formatted_rows.append({\n",
    "                        \"video_id\": c.get(\"video_id\", video_id),\n",
    "                        \"comment_key\": comment_key,\n",
    "                        \"comment_index\": comment_index,\n",
    "                        \"comment_id\": comment_id,\n",
    "                        \"themes\": themes\n",
    "                    })\n",
    "\n",
    "                append_classifications_to_csv(THEMATIC_OUTPUT_CSV, formatted_rows)\n",
    "\n",
    "                idx += cur_batch_size\n",
    "\n",
    "            print(f\"Finished VideoID={video_id}\")\n",
    "\n",
    "        print(f\"\\n=== Processing Complete ===\")\n",
    "        print(f\"Final theme count: {len(theme_dict.get('themes', []))}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping safely…\")\n",
    "        save_checkpoint(CHECKPOINT_PATH, processed_keys)\n",
    "        save_theme_dictionary(THEME_DICT_PATH, theme_dict)\n",
    "    except Exception as e:\n",
    "        save_checkpoint(CHECKPOINT_PATH, processed_keys)\n",
    "        save_theme_dictionary(THEME_DICT_PATH, theme_dict)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18df9d",
   "metadata": {},
   "source": [
    "### 02_Theme Dictionary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4aff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code displaying the theme dictionary contents and counts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
