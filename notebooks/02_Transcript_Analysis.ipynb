{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e5bf69",
   "metadata": {},
   "source": [
    "## File Set-Up\n",
    "- Sets up the Python path to include the repository root, ensuring imports from `src/` work correctly regardless of where the notebook is run\n",
    "- Detects whether the notebook is in a `notebooks/` subdirectory or the repo root, then resolves the correct `repo_root`\n",
    "- Imports shared path constants from `src.paths` (PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR)\n",
    "- Defines notebook-specific input and output file paths using the imported directory constants\n",
    "- Prints the resolved project root and key file paths for verification\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`nb_dir`**: Current working directory of the notebook\n",
    "- **`repo_root`**: Automatically detected repository root (parent of `notebooks/` if applicable)\n",
    "- **Input file**: `TRANSCRIPTS_FILE` = `data/raw/transcripts.json`\n",
    "- **Output files**:\n",
    "    - `TRANSCRIPT_SUMMARY_CSV` = `data/processed/GPT_5_Mini_Transcripts_Summary.csv`\n",
    "    - `TRANSCRIPT_SUMMARY_V2` = `data/processed/GPT_5_Mini_Transcripts_SummaryV2.csv`\n",
    "    - `CHECKPOINT_FILE` = `data/processed/gpt_5_mini_checkpoint.json`\n",
    "\n",
    "## Inputs\n",
    "- **`src/paths.py`**: Module defining `PROJECT_ROOT`, `RAW_DIR`, `PROCESSED_DIR`, `REFERENCE_DIR`, `FIGURES_DIR`\n",
    "- **Expected directory structure**: A `notebooks/` folder (optional) and a `src/` module at the repository root\n",
    "\n",
    "## Outputs\n",
    "- Console output showing:\n",
    "    - Resolved project root path\n",
    "    - Transcripts input file path\n",
    "    - Processed output directory path\n",
    "- No files are written by this cell; it only configures paths for downstream cells\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes `src/paths.py` exists and exports the required path constants\n",
    "- Assumes the repository has either a flat structure or a `notebooks/` subdirectory\n",
    "- Subsequent cells (1, 3, 5) depend on the path variables defined here (`TRANSCRIPTS_FILE`, `TRANSCRIPT_SUMMARY_CSV`, `TRANSCRIPT_SUMMARY_V2`, `CHECKPOINT_FILE`, `PROCESSED_DIR`)\n",
    "- This is a setup cell and must be run before any data processing cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: Ensure repo root is on sys.path for imports ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path.cwd().resolve()\n",
    "repo_root = nb_dir.parent if nb_dir.name == \"notebooks\" else nb_dir\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# === Import shared paths ===\n",
    "from src.paths import PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR\n",
    "\n",
    "# === Notebook-specific file paths ===\n",
    "# Input files\n",
    "TRANSCRIPTS_FILE = RAW_DIR / \"transcripts.json\"\n",
    "\n",
    "# Output files\n",
    "TRANSCRIPT_SUMMARY_CSV = PROCESSED_DIR / \"GPT_5_Mini_Transcripts_Summary.csv\"\n",
    "TRANSCRIPT_SUMMARY_V2 = PROCESSED_DIR / \"GPT_5_Mini_Transcripts_SummaryV2.csv\"\n",
    "CHECKPOINT_FILE = PROCESSED_DIR / \"gpt_5_mini_checkpoint.json\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Transcripts input: {TRANSCRIPTS_FILE}\")\n",
    "print(f\"Processed output dir: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940950f",
   "metadata": {},
   "source": [
    "## 01_Summarize_Transcripts\n",
    "- Loads transcripts from `transcripts.json` and an existing CSV summary (or creates a new one)\n",
    "- Uses OpenAI's GPT-5-mini model to generate single-paragraph summaries for each transcript\n",
    "- Skips videos already summarized or with insufficient content (< 30 words)\n",
    "- Saves progress incrementally after each summary to resume on interruption\n",
    "- Implements a graceful shutdown handler (CTRL+C) that saves all progress before exiting\n",
    "- Adds random 2–6 second delays between API calls to avoid rate limiting\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`CSV_FILE`**: Input CSV file = `data/processed/GPT_5_Mini_Transcripts_Summary.csv`\n",
    "- **`OUTPUT_FILE`**: Output CSV file = `data/processed/GPT_5_Mini_Transcripts_SummaryV2.csv`\n",
    "- **`MODEL_NAME`**: OpenAI model = `\"gpt-5-mini\"`\n",
    "- **`OPENAI_API_KEY`**: Environment variable loaded from `.env` file (required)\n",
    "- **Prompt template**: Enforces a specific summary format via system message\n",
    "\n",
    "## Inputs\n",
    "- **`data/raw/transcripts.json`**: JSON file containing video transcripts with `VideoID` and `Transcript` fields\n",
    "- **`data/processed/GPT_5_Mini_Transcripts_Summary.csv`** (optional): Existing CSV to resume from, or base file to start with\n",
    "- **OpenAI API**: GPT-5-mini chat completions endpoint\n",
    "\n",
    "## Outputs\n",
    "- **`data/processed/GPT_5_Mini_Transcripts_SummaryV2.csv`**: Updated CSV with a `Summary` column containing GPT-generated summaries\n",
    "- Progress is saved after each summary, enabling safe resume on failure or interruption\n",
    "- Console output showing processing status for each video\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes `transcripts.json` contains a list of dictionaries with `VideoID` and `Transcript` keys\n",
    "- Requires the CSV to have or accept a `Summary` column\n",
    "- Skips transcripts with fewer than 30 words or missing video IDs\n",
    "- Depends on path variables (`TRANSCRIPTS_FILE`, `TRANSCRIPT_SUMMARY_CSV`, `TRANSCRIPT_SUMMARY_V2`) defined in cell 1\n",
    "- Uses a signal handler to catch CTRL+C and save progress before exiting\n",
    "- Summaries marked as \"No summary\" indicate insufficient or missing transcript data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# === CONFIG (using portable paths from configuration cell) ===\n",
    "CSV_FILE = TRANSCRIPT_SUMMARY_CSV\n",
    "OUTPUT_FILE = TRANSCRIPT_SUMMARY_V2\n",
    "MODEL_NAME = \"gpt-5-mini\"\n",
    "\n",
    "# === LOAD ENVIRONMENT VARIABLES ===\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Missing OPENAI_API_KEY in .env file\")\n",
    "\n",
    "# === INIT OPENAI CLIENT ===\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# === LOAD TRANSCRIPTS ===\n",
    "with open(TRANSCRIPTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# === LOAD EXISTING OUTPUT OR BASE FILE ===\n",
    "if OUTPUT_FILE.exists():\n",
    "    print(f\"Resuming from existing file: {OUTPUT_FILE}\")\n",
    "    df = pd.read_csv(OUTPUT_FILE)\n",
    "else:\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Ensure there's a Summary column\n",
    "if \"Summary\" not in df.columns:\n",
    "    df[\"Summary\"] = \"\"\n",
    "\n",
    "# === CREATE LOOKUP FOR TRANSCRIPTS ===\n",
    "transcript_lookup = {\n",
    "    item[\"VideoID\"]: item[\"Transcript\"]\n",
    "    for item in transcripts\n",
    "    if \"VideoID\" in item and \"Transcript\" in item\n",
    "}\n",
    "\n",
    "# === SIGNAL HANDLER TO SAVE ON CTRL+C ===\n",
    "def handle_interrupt(signal_received, frame):\n",
    "    print(\"\\nCTRL+C detected — saving progress...\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Progress saved to: {OUTPUT_FILE}\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_interrupt)\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for i, row in df.iterrows():\n",
    "    video_id = row.get(\"VideoID\")\n",
    "\n",
    "    # Skip if already summarized\n",
    "    if pd.notna(row[\"Summary\"]) and len(str(row[\"Summary\"]).strip()) > 10:\n",
    "        continue\n",
    "\n",
    "    if video_id not in transcript_lookup:\n",
    "        print(f\"Skipping {video_id} — transcript not found.\")\n",
    "        df.at[i, \"Summary\"] = \"No summary\"\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "        continue\n",
    "\n",
    "    transcript_text = transcript_lookup[video_id].strip()\n",
    "\n",
    "    # Skip empty or too short transcripts\n",
    "    if len(transcript_text.split()) < 30:\n",
    "        df.at[i, \"Summary\"] = \"No summary\"\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "        print(f\" Skipping {video_id} — not enough information.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Ask GPT-5-mini for a consistent compact paragraph summary\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a precise summarizer that outputs only a single short paragraph. \"\n",
    "                        \"Follow this exact style:\\n\\n\"\n",
    "                        \"This transcript explains [main topic], covering [key technical points]. \"\n",
    "                        \"It discusses [relevant Electrical Engineering aspects] and explains [notable insight]. \"\n",
    "                        \"Overall, it emphasizes [core conclusion or application]. \"\n",
    "                        \"If there is not enough information, respond with exactly: No summary.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Summarize the following transcript:\\n\\n{transcript_text}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Handle empty or malformed outputs\n",
    "        if not summary or summary.lower() == \"no summary\":\n",
    "            summary = \"No summary\"\n",
    "\n",
    "        df.at[i, \"Summary\"] = summary\n",
    "        print(f\" Summarized {video_id}\")\n",
    "\n",
    "        # Save progress after each summary\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # Random delay (2–6 s)\n",
    "        time.sleep(random.uniform(2, 6))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {video_id}: {e}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "# === FINAL SAVE ===\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n All done! Updated file saved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2576d",
   "metadata": {},
   "source": [
    "## 02_Transcript Problem/Solution Extraction + Thematic Analysis\n",
    "- Loads transcripts from `transcripts.json` and extracts problem-solution pairs along with multi-topic percentage classifications using GPT-5-mini\n",
    "- Parses GPT output to identify general topics (e.g., Wiring, Safety, Tools) and subtopics with percentage relevance (e.g., Circuit Design 40%, NEC Compliance 60%)\n",
    "- Normalizes percentages across topics and subtopics to ensure they sum to 100%, correcting for rounding errors\n",
    "- Implements checkpoint-based resume capability to skip already-processed videos and recover from interruptions\n",
    "- Saves results incrementally to CSV after each successful video analysis\n",
    "- Retries failed API calls up to 2 times if invalid output is received\n",
    "- Supports graceful shutdown on keyboard interrupt (CTRL+C), preserving all progress in checkpoint file\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`API_KEY`**: OpenAI API key loaded from `.env` file (required)\n",
    "- **`API_URL`**: OpenAI chat completions endpoint = `\"https://api.openai.com/v1/chat/completions\"`\n",
    "- **`OUTPUT_DIR`**: Output directory = `PROCESSED_DIR` (from cell 1)\n",
    "- **`MODEL_NAME`**: GPT-5-mini (specified in API request)\n",
    "- **`MAX_RETRIES`**: Maximum retry attempts per video = `2`\n",
    "- **Topic taxonomy**: `GENERAL_TOPICS` (8 categories) and `SUBTOPIC_GROUPS` (7 topic families with 5 subtopics each)\n",
    "\n",
    "## Inputs\n",
    "- **`data/raw/transcripts.json`**: JSON file containing video transcripts with `VideoID`, `Title`, `URL`, and `Transcript` fields (via `TRANSCRIPTS_FILE` from cell 1)\n",
    "- **`data/processed/gpt_5_mini_checkpoint.json`** (optional): Checkpoint file to resume from previously processed videos\n",
    "- **OpenAI API**: GPT-5-mini chat completions endpoint for problem/solution extraction and topic classification\n",
    "\n",
    "## Outputs\n",
    "- **`data/processed/GPT_5_Mini_Transcripts_Summary.csv`**: CSV with columns `VideoID`, `Title`, `URL`, `Problem`, `Solution`, `Topics`, `Subtopics` (topics/subtopics formatted as \"Label (X%)\")\n",
    "- **`data/processed/gpt_5_mini_checkpoint.json`**: JSON checkpoint file storing processed video metadata, enabling resume on interruption\n",
    "- Console output showing processing status for each video ID\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes `transcripts.json` contains a list of dictionaries with required keys: `VideoID`, `Title`, `URL`, `Transcript`\n",
    "- Depends on path variables (`TRANSCRIPTS_FILE`, `PROCESSED_DIR`) defined in cell 1\n",
    "- GPT output must follow a strict format with \"Problem:\", \"Solution:\", \"Topics:\", and \"Subtopics:\" sections\n",
    "- Topics and subtopics are extracted via regex matching lines like `- Label (X%)`\n",
    "- CSV is appended to incrementally; existing rows are preserved if the file already exists\n",
    "- Checkpoint file prevents duplicate processing when the script is rerun\n",
    "- The `normalize_percentages` function ensures valid percentage distributions even if GPT output is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5131d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === Output directory (using portable path from configuration cell) ===\n",
    "OUTPUT_DIR = PROCESSED_DIR\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Topic Definitions ===\n",
    "GENERAL_TOPICS = [\n",
    "    \"Wiring\", \"Safety\", \"Tools\", \"Planning\",\n",
    "    \"Education\", \"Technology\", \"Codes & Regulations\", \"Undefined\"\n",
    "]\n",
    "\n",
    "SUBTOPIC_GROUPS = {\n",
    "    \"Wiring\": [\"Residential Wiring\", \"Circuit Design\", \"Conduit Wiring\", \"Panel Wiring\", \"Load Balancing\"],\n",
    "    \"Safety\": [\"Tool Safety\", \"Electrical shock/fire safety\", \"PPE Usage\", \"Hazard Prevention\"],\n",
    "    \"Tools\": [\"Tool Durability\", \"Tool Usage\", \"Measurement Tools\", \"Cutting Tools\", \"Power Tools\"],\n",
    "    \"Planning\": [\"Wiring Planning\", \"Circuit Layout\", \"Load Calculation\", \"Electrical Design\", \"Construction Planning\"],\n",
    "    \"Education\": [\"Apprenticeship\", \"Training\", \"Instruction\", \"Leadership Development\", \"Student Projects\"],\n",
    "    \"Technology\": [\"Modern Tools\", \"Automation\", \"Smart Systems\", \"Prefabrication\", \"Innovation\"],\n",
    "    \"Codes & Regulations\": [\"NEC Compliance\", \"Local Codes\", \"Permitting\", \"Building Codes\", \"Inspection Requirements\"]\n",
    "}\n",
    "\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "\n",
    "# === GPT-5 Request Function ===\n",
    "def ask_openai(transcript):\n",
    "    \"\"\"Send transcript to GPT-5-mini and get multi-topic percentage classification\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    From the following transcript, extract:\n",
    "    1. Problem: The main challenge or issue being addressed.\n",
    "    2. Solution: A concise description of how the problem was solved.\n",
    "    3. Topics: Assign one or more general topics (from {\", \".join(GENERAL_TOPICS)}) \n",
    "       with estimated percentage relevance. \n",
    "       Example: \n",
    "         - Wiring (40%)\n",
    "         - Codes & Regulations (60%)\n",
    "    4. Subtopics: Assign subtopics under each selected topic with their respective percentages.\n",
    "       Example:\n",
    "         - Circuit Design (40%)\n",
    "         - NEC Compliance (60%)\n",
    "\n",
    "    Rules:\n",
    "    - Percentages across topics and subtopics must total approximately 100%.\n",
    "    - Include at least one topic and one subtopic.\n",
    "    - Never use \"Undefined\" unless no suitable label applies.\n",
    "    - Be consistent and concise.\n",
    "\n",
    "    Respond **exactly** in this structure:\n",
    "\n",
    "    Problem: <text>\n",
    "    Solution: <text>\n",
    "    Topics:\n",
    "      - <Topic 1> (<percent>%)\n",
    "      - <Topic 2> (<percent>%)\n",
    "    Subtopics:\n",
    "      - <Subtopic 1> (<percent>%)\n",
    "      - <Subtopic 2> (<percent>%)\n",
    "\n",
    "    Transcript:\n",
    "    {transcript}\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-5-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that classifies electrical engineering transcripts into multiple topics with percentage relevance.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        res_json = response.json()\n",
    "        return res_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# === Result Parsing ===\n",
    "def parse_result(result):\n",
    "    \"\"\"Parse GPT output into structured components\"\"\"\n",
    "    problem, solution = \"Not Extracted\", \"Not Extracted\"\n",
    "    topics, subtopics = [], []\n",
    "\n",
    "    lines = result.splitlines()\n",
    "    section = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"problem\"):\n",
    "            problem = line.split(\":\", 1)[-1].strip()\n",
    "        elif line.lower().startswith(\"solution\"):\n",
    "            solution = line.split(\":\", 1)[-1].strip()\n",
    "        elif line.lower().startswith(\"topics\"):\n",
    "            section = \"topics\"\n",
    "        elif line.lower().startswith(\"subtopics\"):\n",
    "            section = \"subtopics\"\n",
    "        elif line.strip().startswith(\"-\"):\n",
    "            match = re.match(r\"-\\s*(.+?)\\s*\\((\\d+)%\\)\", line.strip())\n",
    "            if match:\n",
    "                label, percent = match.groups()\n",
    "                percent = int(percent)\n",
    "                if section == \"topics\":\n",
    "                    topics.append((label, percent))\n",
    "                elif section == \"subtopics\":\n",
    "                    subtopics.append((label, percent))\n",
    "\n",
    "    # Normalize percentages so they sum to 100\n",
    "    topics = normalize_percentages(topics)\n",
    "    subtopics = normalize_percentages(subtopics)\n",
    "\n",
    "    return problem, solution, topics, subtopics\n",
    "\n",
    "\n",
    "def normalize_percentages(pairs):\n",
    "    \"\"\"Ensure percentages sum to 100 and fix rounding errors\"\"\"\n",
    "    if not pairs:\n",
    "        return []\n",
    "\n",
    "    total = sum(p[1] for p in pairs)\n",
    "    if total == 0:\n",
    "        equal_share = round(100 / len(pairs))\n",
    "        return [(p[0], equal_share) for p in pairs]\n",
    "\n",
    "    normalized = [(p[0], round(p[1] * 100 / total)) for p in pairs]\n",
    "    diff = 100 - sum(p[1] for p in normalized)\n",
    "    if diff != 0:\n",
    "        first = list(normalized[0])\n",
    "        first[1] += diff\n",
    "        normalized[0] = tuple(first)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# === Main Transcript Processing ===\n",
    "def process_transcripts(json_file, output_csv, checkpoint_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcripts = json.load(f)\n",
    "\n",
    "    checkpoint_data = {}\n",
    "    if checkpoint_file.exists():\n",
    "        with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "\n",
    "    write_header = not output_csv.exists()\n",
    "    with open(output_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if write_header:\n",
    "            writer.writerow([\"VideoID\", \"Title\", \"URL\", \"Problem\", \"Solution\", \"Topics\", \"Subtopics\"])\n",
    "\n",
    "        for item in transcripts:\n",
    "            vid = item[\"VideoID\"]\n",
    "            if vid in checkpoint_data:\n",
    "                print(f\"Skipping Video {vid} (already processed).\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing Video {vid}: {item['Title'][:50]}...\")\n",
    "\n",
    "            retries = 0\n",
    "            while retries <= MAX_RETRIES:\n",
    "                result = ask_openai(item[\"Transcript\"])\n",
    "                if not result:\n",
    "                    retries += 1\n",
    "                    continue\n",
    "\n",
    "                problem, solution, topics, subtopics = parse_result(result)\n",
    "                if not topics:\n",
    "                    retries += 1\n",
    "                    print(f\"Retry {retries} for Video {vid} due to invalid output...\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to readable strings for CSV\n",
    "                topics_str = \", \".join([f\"{t} ({p}%)\" for t, p in topics])\n",
    "                subtopics_str = \", \".join([f\"{t} ({p}%)\" for t, p in subtopics])\n",
    "\n",
    "                writer.writerow([vid, item[\"Title\"], item[\"URL\"], problem, solution, topics_str, subtopics_str])\n",
    "                csvfile.flush()\n",
    "\n",
    "                checkpoint_data[vid] = {\n",
    "                    \"Title\": item[\"Title\"],\n",
    "                    \"URL\": item[\"URL\"],\n",
    "                    \"Problem\": problem,\n",
    "                    \"Solution\": solution,\n",
    "                    \"Topics\": topics,\n",
    "                    \"Subtopics\": subtopics\n",
    "                }\n",
    "                with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "                print(f\"Saved Video {vid}\")\n",
    "                break\n",
    "\n",
    "\n",
    "# === Run Script ===\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_transcripts(\n",
    "            json_file=TRANSCRIPTS_FILE,\n",
    "            output_csv=OUTPUT_DIR / \"GPT_5_Mini_Transcripts_Summary.csv\",\n",
    "            checkpoint_file=OUTPUT_DIR / \"gpt_5_mini_checkpoint.json\"\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Interrupted by user. Progress saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c1a56",
   "metadata": {},
   "source": [
    "### 03_Transcript Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code showing examples from the transcript analysis\n",
    "# Some basic visualizations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
