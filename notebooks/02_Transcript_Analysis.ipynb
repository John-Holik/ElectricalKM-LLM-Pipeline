{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e5bf69",
   "metadata": {},
   "source": [
    "## File Set-Up\n",
    "- Sets up the Python path to include the repository root, ensuring imports from `src/` work correctly regardless of where the notebook is run\n",
    "- Detects whether the notebook is in a `notebooks/` subdirectory or the repo root, then resolves the correct `repo_root`\n",
    "- Imports shared path constants from `src.paths` (PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR)\n",
    "- Defines notebook-specific input and output file paths using the imported directory constants\n",
    "- Prints the resolved project root and key file paths for verification\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`nb_dir`**: Current working directory of the notebook\n",
    "- **`repo_root`**: Automatically detected repository root (parent of `notebooks/` if applicable)\n",
    "- **Input file**: `TRANSCRIPTS_FILE` = `data/raw/transcripts.json`\n",
    "- **Output files**:\n",
    "    - `TRANSCRIPT_SUMMARY_CSV` = `data/processed/Transcript_Summary.csv` (Step 1 output)\n",
    "    - `TRANSCRIPT_SUMMARY_V2` = `data/processed/Transcript_SummaryV2.csv` (Step 2 output)\n",
    "    - `CHECKPOINT_FILE` = `data/processed/transcript_summary_checkpoint.json`\n",
    "    - `CHECKPOINT_FILE_V2` = `data/processed/transcript_analysis_checkpoint.json`\n",
    "\n",
    "## Inputs\n",
    "- **`src/paths.py`**: Module defining `PROJECT_ROOT`, `RAW_DIR`, `PROCESSED_DIR`, `REFERENCE_DIR`, `FIGURES_DIR`\n",
    "- **Expected directory structure**: A `notebooks/` folder (optional) and a `src/` module at the repository root\n",
    "\n",
    "## Outputs\n",
    "- Console output showing:\n",
    "    - Resolved project root path\n",
    "    - Transcripts input file path\n",
    "    - Processed output directory path\n",
    "- No files are written by this cell; it only configures paths for downstream cells\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes `src/paths.py` exists and exports the required path constants\n",
    "- Assumes the repository has either a flat structure or a `notebooks/` subdirectory\n",
    "- Subsequent cells depend on the path variables defined here\n",
    "- This is a setup cell and must be run before any data processing cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: Ensure repo root is on sys.path for imports ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path.cwd().resolve()\n",
    "repo_root = nb_dir.parent if nb_dir.name == \"notebooks\" else nb_dir\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# === Import shared paths ===\n",
    "from src.paths import PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR\n",
    "\n",
    "# === Notebook-specific file paths ===\n",
    "# Input files\n",
    "TRANSCRIPTS_FILE = RAW_DIR / \"transcripts.json\"\n",
    "\n",
    "# Output files\n",
    "TRANSCRIPT_SUMMARY_CSV = PROCESSED_DIR / \"Transcript_Summary.csv\"  # Step 1 output\n",
    "TRANSCRIPT_SUMMARY_V2 = PROCESSED_DIR / \"Transcript_SummaryV2.csv\"  # Step 2 output\n",
    "CHECKPOINT_FILE = PROCESSED_DIR / \"transcript_summary_checkpoint.json\"  # Step 1 checkpoint\n",
    "CHECKPOINT_FILE_V2 = PROCESSED_DIR / \"transcript_analysis_checkpoint.json\"  # Step 2 checkpoint\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Transcripts input: {TRANSCRIPTS_FILE}\")\n",
    "print(f\"Step 1 output: {TRANSCRIPT_SUMMARY_CSV}\")\n",
    "print(f\"Step 2 output: {TRANSCRIPT_SUMMARY_V2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940950f",
   "metadata": {},
   "source": [
    "## 01_Summarize_Transcripts\n",
    "- Loads transcripts from `transcripts.json` and extracts metadata + generates summaries\n",
    "- Uses OpenAI's GPT-5-mini model to generate single-paragraph summaries for each transcript\n",
    "- Extracts VideoID, Title, URL, ViewCount, and LikeCount from the JSON data\n",
    "- Skips videos already processed or with insufficient content (< 30 words)\n",
    "- Saves progress incrementally after each summary to resume on interruption\n",
    "- Implements a graceful shutdown handler (CTRL+C) that saves all progress before exiting\n",
    "- Adds random 2–6 second delays between API calls to avoid rate limiting\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`OUTPUT_FILE`**: Output CSV file = `data/processed/Transcript_Summary.csv`\n",
    "- **`MODEL_NAME`**: OpenAI model = `\"gpt-5-mini\"`\n",
    "- **`OPENAI_API_KEY`**: Environment variable loaded from `.env` file (required)\n",
    "- **Prompt template**: Enforces a specific summary format via system message\n",
    "\n",
    "## Inputs\n",
    "- **`data/raw/transcripts.json`**: JSON file containing video transcripts with `VideoID`, `Title`, `URL`, `ViewCount`, `LikeCount`, and `Transcript` fields\n",
    "- **`data/processed/transcript_summary_checkpoint.json`** (optional): Checkpoint file to resume from\n",
    "- **OpenAI API**: GPT-5-mini chat completions endpoint\n",
    "\n",
    "## Outputs\n",
    "- **`data/processed/Transcript_Summary.csv`**: CSV with columns `VideoID`, `Title`, `URL`, `ViewCount`, `LikeCount`, `Summary`\n",
    "- **`data/processed/transcript_summary_checkpoint.json`**: Checkpoint file tracking processed videos\n",
    "- Console output showing processing status for each video\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes `transcripts.json` contains a list of dictionaries with `VideoID`, `Title`, `URL`, `ViewCount`, `LikeCount`, and `Transcript` keys\n",
    "- Skips transcripts with fewer than 30 words or missing video IDs\n",
    "- Depends on path variables defined in cell 1\n",
    "- Uses a signal handler to catch CTRL+C and save progress before exiting\n",
    "- Summaries marked as \"No summary\" indicate insufficient or missing transcript data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# === CONFIG (using portable paths from configuration cell) ===\n",
    "OUTPUT_FILE = TRANSCRIPT_SUMMARY_CSV\n",
    "MODEL_NAME = \"gpt-5-mini\"\n",
    "\n",
    "# === LOAD ENVIRONMENT VARIABLES ===\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Missing OPENAI_API_KEY in .env file\")\n",
    "\n",
    "# === INIT OPENAI CLIENT ===\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# === LOAD TRANSCRIPTS ===\n",
    "with open(TRANSCRIPTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# === LOAD CHECKPOINT ===\n",
    "checkpoint_data = {}\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "    print(f\"Resuming from checkpoint with {len(checkpoint_data)} processed videos\")\n",
    "\n",
    "# === SIGNAL HANDLER TO SAVE ON CTRL+C ===\n",
    "def handle_interrupt(signal_received, frame):\n",
    "    print(\"\\nCTRL+C detected — saving progress...\")\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Progress saved to: {CHECKPOINT_FILE}\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_interrupt)\n",
    "\n",
    "# === INITIALIZE OUTPUT CSV ===\n",
    "write_header = not OUTPUT_FILE.exists()\n",
    "csvfile = open(OUTPUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "writer = csv.writer(csvfile)\n",
    "if write_header:\n",
    "    writer.writerow([\"VideoID\", \"Title\", \"URL\", \"ViewCount\", \"LikeCount\", \"Summary\"])\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "try:\n",
    "    for item in transcripts:\n",
    "        video_id = item.get(\"VideoID\")\n",
    "        \n",
    "        if not video_id:\n",
    "            print(\"Skipping item — no VideoID found.\")\n",
    "            continue\n",
    "\n",
    "        # Skip if already processed\n",
    "        if video_id in checkpoint_data:\n",
    "            continue\n",
    "\n",
    "        title = item.get(\"Title\", \"\")\n",
    "        url = item.get(\"URL\", \"\")\n",
    "        view_count = item.get(\"ViewCount\", 0)\n",
    "        like_count = item.get(\"LikeCount\", 0)\n",
    "        transcript_text = item.get(\"Transcript\", \"\").strip()\n",
    "\n",
    "        # Skip empty or too short transcripts\n",
    "        if len(transcript_text.split()) < 30:\n",
    "            summary = \"No summary\"\n",
    "            writer.writerow([video_id, title, url, view_count, like_count, summary])\n",
    "            csvfile.flush()\n",
    "            checkpoint_data[video_id] = {\"Summary\": summary}\n",
    "            with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"Skipping {video_id} — not enough information.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Ask GPT-5-mini for a consistent compact paragraph summary\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"You are a precise summarizer that outputs only a single short paragraph. \"\n",
    "                            \"Follow this exact style:\\n\\n\"\n",
    "                            \"This transcript explains [main topic], covering [key technical points]. \"\n",
    "                            \"It discusses [relevant Electrical Engineering aspects] and explains [notable insight]. \"\n",
    "                            \"Overall, it emphasizes [core conclusion or application]. \"\n",
    "                            \"If there is not enough information, respond with exactly: No summary.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Summarize the following transcript:\\n\\n{transcript_text}\",\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Handle empty or malformed outputs\n",
    "            if not summary or summary.lower() == \"no summary\":\n",
    "                summary = \"No summary\"\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow([video_id, title, url, view_count, like_count, summary])\n",
    "            csvfile.flush()\n",
    "\n",
    "            # Update checkpoint\n",
    "            checkpoint_data[video_id] = {\"Summary\": summary}\n",
    "            with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(f\"Summarized {video_id}\")\n",
    "\n",
    "            # Random delay (2–6 s)\n",
    "            time.sleep(random.uniform(2, 6))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing {video_id}: {e}\")\n",
    "            time.sleep(10)\n",
    "\n",
    "finally:\n",
    "    csvfile.close()\n",
    "\n",
    "print(f\"\\nStep 1 complete! Output saved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2576d",
   "metadata": {},
   "source": [
    "## 02_Problem/Solution and Topic Extraction\n",
    "- Loads `Transcript_Summary.csv` from Step 1 and the original transcripts from `transcripts.json`\n",
    "- Uses GPT-5-mini to extract Problem, Solution, Topics, and Subtopics from each full transcript\n",
    "- Adds these four new columns to create `Transcript_SummaryV2.csv` (preserving all existing columns)\n",
    "- Parses GPT output to identify topics and subtopics with percentage relevance\n",
    "- Normalizes percentages across topics and subtopics to ensure they sum to 100%\n",
    "- Implements checkpoint-based resume capability to skip already-processed videos\n",
    "- Saves results incrementally to avoid data loss on interruption\n",
    "- Retries failed API calls up to 2 times if invalid output is received\n",
    "\n",
    "## Configuration / Setup\n",
    "- **`INPUT_FILE`**: Input CSV = `data/processed/Transcript_Summary.csv` (from Step 1)\n",
    "- **`OUTPUT_FILE`**: Output CSV = `data/processed/Transcript_SummaryV2.csv`\n",
    "- **`MODEL_NAME`**: GPT-5-mini (specified in API request)\n",
    "- **`MAX_RETRIES`**: Maximum retry attempts per video = `2`\n",
    "- **Topic taxonomy**: `GENERAL_TOPICS` (8 categories) and `SUBTOPIC_GROUPS` (7 topic families with subtopics)\n",
    "\n",
    "## Inputs\n",
    "- **`data/processed/Transcript_Summary.csv`**: CSV with VideoID, Title, URL, ViewCount, LikeCount, Summary (from Step 1)\n",
    "- **`data/raw/transcripts.json`**: JSON file containing full transcripts for Problem/Solution extraction\n",
    "- **`data/processed/transcript_analysis_checkpoint.json`** (optional): Checkpoint file to resume from\n",
    "- **OpenAI API**: GPT-5-mini chat completions endpoint\n",
    "\n",
    "## Outputs\n",
    "- **`data/processed/Transcript_SummaryV2.csv`**: CSV with all Step 1 columns plus `Problem`, `Solution`, `Topics`, `Subtopics`\n",
    "- **`data/processed/transcript_analysis_checkpoint.json`**: Checkpoint file tracking processed videos\n",
    "- Console output showing processing status for each video ID\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Requires Step 1 (`Transcript_Summary.csv`) to be completed first\n",
    "- Uses full transcript from JSON for extraction (not the summary)\n",
    "- Depends on path variables defined in cell 1\n",
    "- Topics and subtopics are formatted as \"Label (X%)\" strings in the CSV\n",
    "- The `normalize_percentages` function ensures valid percentage distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5131d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "# === CONFIG (using portable paths from configuration cell) ===\n",
    "INPUT_FILE = TRANSCRIPT_SUMMARY_CSV  # Step 1 output\n",
    "OUTPUT_FILE = TRANSCRIPT_SUMMARY_V2  # Step 2 output\n",
    "MODEL_NAME = \"gpt-5-mini\"\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "# === LOAD ENVIRONMENT VARIABLES ===\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"Missing OPENAI_API_KEY in .env file\")\n",
    "\n",
    "# === INIT OPENAI CLIENT ===\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# === Topic Definitions ===\n",
    "GENERAL_TOPICS = [\n",
    "    \"Wiring\", \"Safety\", \"Tools\", \"Planning\",\n",
    "    \"Education\", \"Technology\", \"Codes & Regulations\", \"Undefined\"\n",
    "]\n",
    "\n",
    "SUBTOPIC_GROUPS = {\n",
    "    \"Wiring\": [\"Residential Wiring\", \"Circuit Design\", \"Conduit Wiring\", \"Panel Wiring\", \"Load Balancing\"],\n",
    "    \"Safety\": [\"Tool Safety\", \"Electrical shock/fire safety\", \"PPE Usage\", \"Hazard Prevention\"],\n",
    "    \"Tools\": [\"Tool Durability\", \"Tool Usage\", \"Measurement Tools\", \"Cutting Tools\", \"Power Tools\"],\n",
    "    \"Planning\": [\"Wiring Planning\", \"Circuit Layout\", \"Load Calculation\", \"Electrical Design\", \"Construction Planning\"],\n",
    "    \"Education\": [\"Apprenticeship\", \"Training\", \"Instruction\", \"Leadership Development\", \"Student Projects\"],\n",
    "    \"Technology\": [\"Modern Tools\", \"Automation\", \"Smart Systems\", \"Prefabrication\", \"Innovation\"],\n",
    "    \"Codes & Regulations\": [\"NEC Compliance\", \"Local Codes\", \"Permitting\", \"Building Codes\", \"Inspection Requirements\"]\n",
    "}\n",
    "\n",
    "\n",
    "# === GPT Request Function ===\n",
    "def extract_problem_solution_topics(transcript_text):\n",
    "    \"\"\"Send transcript to GPT-5-mini and get problem/solution + topic classification\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "From the following transcript, extract:\n",
    "1. Problem: The main challenge or issue being addressed.\n",
    "2. Solution: A concise description of how the problem was solved.\n",
    "3. Topics: Assign one or more general topics (from {\", \".join(GENERAL_TOPICS)}) \n",
    "   with estimated percentage relevance. \n",
    "   Example: \n",
    "     - Wiring (40%)\n",
    "     - Codes & Regulations (60%)\n",
    "4. Subtopics: Assign subtopics under each selected topic with their respective percentages.\n",
    "   Example:\n",
    "     - Circuit Design (40%)\n",
    "     - NEC Compliance (60%)\n",
    "\n",
    "Rules:\n",
    "- Percentages across topics and subtopics must total approximately 100%.\n",
    "- Include at least one topic and one subtopic.\n",
    "- Never use \"Undefined\" unless no suitable label applies.\n",
    "- Be consistent and concise.\n",
    "\n",
    "Respond **exactly** in this structure:\n",
    "\n",
    "Problem: <text>\n",
    "Solution: <text>\n",
    "Topics:\n",
    "  - <Topic 1> (<percent>%)\n",
    "  - <Topic 2> (<percent>%)\n",
    "Subtopics:\n",
    "  - <Subtopic 1> (<percent>%)\n",
    "  - <Subtopic 2> (<percent>%)\n",
    "\n",
    "Transcript:\n",
    "{transcript_text}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that extracts problems, solutions, and classifies electrical engineering transcripts into multiple topics with percentage relevance.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# === Result Parsing ===\n",
    "def parse_result(result):\n",
    "    \"\"\"Parse GPT output into structured components\"\"\"\n",
    "    problem, solution = \"Not Extracted\", \"Not Extracted\"\n",
    "    topics, subtopics = [], []\n",
    "\n",
    "    lines = result.splitlines()\n",
    "    section = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line.lower().startswith(\"problem\"):\n",
    "            problem = line.split(\":\", 1)[-1].strip()\n",
    "        elif line.lower().startswith(\"solution\"):\n",
    "            solution = line.split(\":\", 1)[-1].strip()\n",
    "        elif line.lower().startswith(\"topics\"):\n",
    "            section = \"topics\"\n",
    "        elif line.lower().startswith(\"subtopics\"):\n",
    "            section = \"subtopics\"\n",
    "        elif line.strip().startswith(\"-\"):\n",
    "            match = re.match(r\"-\\s*(.+?)\\s*\\((\\d+)%\\)\", line.strip())\n",
    "            if match:\n",
    "                label, percent = match.groups()\n",
    "                percent = int(percent)\n",
    "                if section == \"topics\":\n",
    "                    topics.append((label, percent))\n",
    "                elif section == \"subtopics\":\n",
    "                    subtopics.append((label, percent))\n",
    "\n",
    "    # Normalize percentages so they sum to 100\n",
    "    topics = normalize_percentages(topics)\n",
    "    subtopics = normalize_percentages(subtopics)\n",
    "\n",
    "    return problem, solution, topics, subtopics\n",
    "\n",
    "\n",
    "def normalize_percentages(pairs):\n",
    "    \"\"\"Ensure percentages sum to 100 and fix rounding errors\"\"\"\n",
    "    if not pairs:\n",
    "        return []\n",
    "\n",
    "    total = sum(p[1] for p in pairs)\n",
    "    if total == 0:\n",
    "        equal_share = round(100 / len(pairs))\n",
    "        return [(p[0], equal_share) for p in pairs]\n",
    "\n",
    "    normalized = [(p[0], round(p[1] * 100 / total)) for p in pairs]\n",
    "    diff = 100 - sum(p[1] for p in normalized)\n",
    "    if diff != 0:\n",
    "        first = list(normalized[0])\n",
    "        first[1] += diff\n",
    "        normalized[0] = tuple(first)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# === LOAD TRANSCRIPTS JSON ===\n",
    "with open(TRANSCRIPTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create lookup for transcripts\n",
    "transcript_lookup = {\n",
    "    item[\"VideoID\"]: item[\"Transcript\"]\n",
    "    for item in transcripts\n",
    "    if \"VideoID\" in item and \"Transcript\" in item\n",
    "}\n",
    "\n",
    "# === LOAD STEP 1 CSV ===\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Step 1 output not found: {INPUT_FILE}. Run Step 1 first.\")\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded {len(df)} rows from Step 1 output\")\n",
    "\n",
    "# === LOAD CHECKPOINT ===\n",
    "checkpoint_data = {}\n",
    "if CHECKPOINT_FILE_V2.exists():\n",
    "    with open(CHECKPOINT_FILE_V2, \"r\", encoding=\"utf-8\") as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "    print(f\"Resuming from checkpoint with {len(checkpoint_data)} processed videos\")\n",
    "\n",
    "# === Add new columns if they don't exist ===\n",
    "for col in [\"Problem\", \"Solution\", \"Topics\", \"Subtopics\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# === SIGNAL HANDLER TO SAVE ON CTRL+C ===\n",
    "def handle_interrupt(signal_received, frame):\n",
    "    print(\"\\nCTRL+C detected — saving progress...\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "    with open(CHECKPOINT_FILE_V2, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Progress saved to: {OUTPUT_FILE}\")\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_interrupt)\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "try:\n",
    "    for i, row in df.iterrows():\n",
    "        video_id = row.get(\"VideoID\")\n",
    "        \n",
    "        if not video_id:\n",
    "            continue\n",
    "\n",
    "        # Skip if already processed\n",
    "        if str(video_id) in checkpoint_data:\n",
    "            # Restore from checkpoint\n",
    "            df.at[i, \"Problem\"] = checkpoint_data[str(video_id)].get(\"Problem\", \"\")\n",
    "            df.at[i, \"Solution\"] = checkpoint_data[str(video_id)].get(\"Solution\", \"\")\n",
    "            df.at[i, \"Topics\"] = checkpoint_data[str(video_id)].get(\"Topics\", \"\")\n",
    "            df.at[i, \"Subtopics\"] = checkpoint_data[str(video_id)].get(\"Subtopics\", \"\")\n",
    "            continue\n",
    "\n",
    "        # Get full transcript from JSON\n",
    "        if video_id not in transcript_lookup:\n",
    "            print(f\"Skipping {video_id} — transcript not found in JSON.\")\n",
    "            df.at[i, \"Problem\"] = \"Not Extracted\"\n",
    "            df.at[i, \"Solution\"] = \"Not Extracted\"\n",
    "            df.at[i, \"Topics\"] = \"\"\n",
    "            df.at[i, \"Subtopics\"] = \"\"\n",
    "            continue\n",
    "\n",
    "        transcript_text = transcript_lookup[video_id].strip()\n",
    "\n",
    "        # Skip empty or too short transcripts\n",
    "        if len(transcript_text.split()) < 30:\n",
    "            df.at[i, \"Problem\"] = \"Not Extracted\"\n",
    "            df.at[i, \"Solution\"] = \"Not Extracted\"\n",
    "            df.at[i, \"Topics\"] = \"\"\n",
    "            df.at[i, \"Subtopics\"] = \"\"\n",
    "            checkpoint_data[str(video_id)] = {\n",
    "                \"Problem\": \"Not Extracted\",\n",
    "                \"Solution\": \"Not Extracted\",\n",
    "                \"Topics\": \"\",\n",
    "                \"Subtopics\": \"\"\n",
    "            }\n",
    "            print(f\"Skipping {video_id} — transcript too short.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {video_id}...\")\n",
    "\n",
    "        retries = 0\n",
    "        success = False\n",
    "        while retries <= MAX_RETRIES and not success:\n",
    "            try:\n",
    "                result = extract_problem_solution_topics(transcript_text)\n",
    "                problem, solution, topics, subtopics = parse_result(result)\n",
    "\n",
    "                if not topics:\n",
    "                    retries += 1\n",
    "                    print(f\"Retry {retries} for {video_id} — no topics extracted.\")\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "\n",
    "                # Convert to readable strings\n",
    "                topics_str = \", \".join([f\"{t} ({p}%)\" for t, p in topics])\n",
    "                subtopics_str = \", \".join([f\"{t} ({p}%)\" for t, p in subtopics])\n",
    "\n",
    "                # Update dataframe\n",
    "                df.at[i, \"Problem\"] = problem\n",
    "                df.at[i, \"Solution\"] = solution\n",
    "                df.at[i, \"Topics\"] = topics_str\n",
    "                df.at[i, \"Subtopics\"] = subtopics_str\n",
    "\n",
    "                # Update checkpoint\n",
    "                checkpoint_data[str(video_id)] = {\n",
    "                    \"Problem\": problem,\n",
    "                    \"Solution\": solution,\n",
    "                    \"Topics\": topics_str,\n",
    "                    \"Subtopics\": subtopics_str\n",
    "                }\n",
    "                with open(CHECKPOINT_FILE_V2, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "                # Save CSV incrementally\n",
    "                df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "                print(f\"Extracted {video_id}\")\n",
    "                success = True\n",
    "\n",
    "                # Random delay\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_id}: {e}\")\n",
    "                retries += 1\n",
    "                time.sleep(5)\n",
    "\n",
    "        if not success:\n",
    "            df.at[i, \"Problem\"] = \"Not Extracted\"\n",
    "            df.at[i, \"Solution\"] = \"Not Extracted\"\n",
    "            df.at[i, \"Topics\"] = \"\"\n",
    "            df.at[i, \"Subtopics\"] = \"\"\n",
    "\n",
    "finally:\n",
    "    # Final save\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "    with open(CHECKPOINT_FILE_V2, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nStep 2 complete! Output saved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c1a56",
   "metadata": {},
   "source": [
    "### 03_Transcript Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code showing examples from the transcript analysis\n",
    "# Some basic visualizations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
