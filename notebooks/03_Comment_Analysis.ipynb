{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d97b56",
   "metadata": {},
   "source": [
    "## File Set-Up\n",
    "- Sets up Python module search path by adding the repository root to `sys.path` for reliable imports\n",
    "- Imports shared directory paths from `src.paths` module (PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR)\n",
    "- Defines input file paths: raw YouTube comments JSON, comments with transcripts JSON, and analyzed transcripts CSV\n",
    "- Defines output file paths: comment analysis CSV and state file for resumable processing\n",
    "- Prints configuration summary showing project root and key input/output paths\n",
    "\n",
    "## Configuration / Setup\n",
    "- **nb_dir**: Current working directory (auto-detected)\n",
    "- **repo_root**: Repository root directory (parent of `notebooks/` if present, else current directory)\n",
    "- **COMMENTS_JSON**: `RAW_DIR / \"youtube_comments.json\"` → `data/raw/youtube_comments.json`\n",
    "- **COMMENTS_WITH_TRANSCRIPTS_JSON**: `PROCESSED_DIR / \"comments_with_transcripts.json\"` → `data/processed/comments_with_transcripts.json`\n",
    "- **TRANSCRIPTS_CSV**: `PROCESSED_DIR / \"GPT_5_Mini_Transcripts_SummaryV2.csv\"` → `data/processed/GPT_5_Mini_Transcripts_SummaryV2.csv`\n",
    "- **COMMENT_ANALYSIS_CSV**: `PROCESSED_DIR / \"GPT_5_Mini_Comment_Analysis.csv\"` → `data/processed/GPT_5_Mini_Comment_Analysis.csv`\n",
    "- **STATE_FILE**: `PROCESSED_DIR / \"comments_analysis_state.json\"` → `data/processed/comments_analysis_state.json`\n",
    "\n",
    "## Inputs\n",
    "- **src.paths module**: Must exist at repository root and export PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR\n",
    "- **youtube_comments.json** (optional): Raw YouTube comments data in `data/raw/`\n",
    "- **comments_with_transcripts.json** (optional): Enriched comments with transcript context in `data/processed/`\n",
    "- **GPT_5_Mini_Transcripts_SummaryV2.csv**: Analyzed video transcripts with summaries in `data/processed/`\n",
    "\n",
    "## Outputs\n",
    "- **Console output**: Prints project root path and key file paths for verification\n",
    "- No files are created or modified by this cell (configuration only)\n",
    "\n",
    "## Notes / Assumptions\n",
    "- Assumes standard project structure with `notebooks/` subdirectory (if present) and `src/` module at repo root\n",
    "- Expects `src/paths.py` to define and export all directory path constants\n",
    "- The cell is purely declarative—no data processing occurs; it only sets up variables for subsequent cells\n",
    "- Subsequent cells (e.g., CELL INDEX 3) depend on these path variables being defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: Ensure repo root is on sys.path for imports ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path.cwd().resolve()\n",
    "repo_root = nb_dir.parent if nb_dir.name == \"notebooks\" else nb_dir\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# === Import shared paths ===\n",
    "from src.paths import PROJECT_ROOT, RAW_DIR, PROCESSED_DIR, REFERENCE_DIR, FIGURES_DIR\n",
    "\n",
    "# === Notebook-specific file paths ===\n",
    "# Input files\n",
    "COMMENTS_JSON = RAW_DIR / \"youtube_comments.json\"\n",
    "COMMENTS_WITH_TRANSCRIPTS_JSON = PROCESSED_DIR / \"comments_with_transcripts.json\"\n",
    "TRANSCRIPTS_CSV = PROCESSED_DIR / \"GPT_5_Mini_Transcripts_SummaryV2.csv\"\n",
    "\n",
    "# Output files\n",
    "COMMENT_ANALYSIS_CSV = PROCESSED_DIR / \"GPT_5_Mini_Comment_Analysis.csv\"\n",
    "STATE_FILE = PROCESSED_DIR / \"comments_analysis_state.json\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Comments input: {COMMENTS_JSON}\")\n",
    "print(f\"Comment analysis output: {COMMENT_ANALYSIS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9136d",
   "metadata": {},
   "source": [
    "## 01_Comment Analysis with context from Analyzed Transcripts\n",
    "\n",
    "- Processes YouTube comment threads in parallel using OpenAI GPT to extract structured electrical engineering insights\n",
    "- Filters parent comments to only analyze those with ≥4 tokens (using tiktoken or whitespace fallback)\n",
    "- For each eligible parent comment, sends video context (title + transcript summary) and comment thread to GPT with a strict JSON schema prompt\n",
    "- GPT extracts question/answer excerpts and summaries, overall reply summary, and electrical engineering topic/subtopic breakdowns (percentages summing to 100)\n",
    "- Writes analysis results to CSV with 10 columns: VideoID, CommentID, QuestionExcerpt, QuestionSummary, AnswerExcerpt, AnswerSummary, ReplySummary, ReplyCount, Topic, Sub-Topic\n",
    "- Implements graceful CTRL+C handling: saves state after each row, allowing resumable processing by skipping already-analyzed (VideoID, CommentID) pairs\n",
    "- Uses a dedicated writer thread to serialize CSV writes and state persistence, while video workers run in parallel (configurable via `VIDEO_WORKERS`)\n",
    "- Includes retry logic with exponential backoff for OpenAI API calls (up to 5 retries) and random jitter between calls to avoid rate limits\n",
    "\n",
    "## Configuration / Setup\n",
    "\n",
    "- **OPENAI_API_KEY**: Must be set in environment (`.env` or exported); required for GPT API calls\n",
    "- **MODEL_NAME**: Defaults to `\"gpt-5-mini\"` (override with `OPENAI_MODEL` env var)\n",
    "- **VIDEO_WORKERS**: Parallel video processing threads (default 4; set via `VIDEO_WORKERS` env var)\n",
    "- **MIN_PARENT_TOKENS**: Minimum token count for parent comments (default 4)\n",
    "- **USE_TIKTOKEN**: If `True`, uses tiktoken `cl100k_base` encoding; if `False` or tiktoken unavailable, falls back to whitespace split\n",
    "- **MAX_RETRIES**: API retry limit (default 5)\n",
    "- **MIN_DELAY_S / MAX_DELAY_S**: Random jitter range between API calls (0.2–0.6 seconds)\n",
    "- **OUTPUT_CSV**: Resolves to `COMMENT_ANALYSIS_CSV` from cell 1 → `data/processed/GPT_5_Mini_Comment_Analysis.csv`\n",
    "- **STATE_FILE**: Resolves to `data/processed/comments_analysis_state.json` (for resumable processing)\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- **COMMENTS_WITH_TRANSCRIPTS_JSON** (`data/processed/comments_with_transcripts.json`): Preferred input; enriched comments with transcript context\n",
    "    - Falls back to **COMMENTS_JSON** (`data/raw/youtube_comments.json`) if not found\n",
    "- **TRANSCRIPTS_CSV** (`data/processed/GPT_5_Mini_Transcripts_SummaryV2.csv`): Video transcript summaries; must contain a VideoID column and a column with \"summary\" in the name\n",
    "- **OpenAI API**: GPT model accessed via `openai` Python client using `OPENAI_API_KEY`\n",
    "- **Existing output CSV** (if present): Read to identify already-processed (VideoID, CommentID) pairs for resumption\n",
    "- **State file** (`comments_analysis_state.json`): Read at startup to resume from previous runs\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- **GPT_5_Mini_Comment_Analysis.csv** (`data/processed/GPT_5_Mini_Comment_Analysis.csv`): Main output with 10 columns per comment:\n",
    "    - `VideoID`, `CommentID`, `QuestionExcerpt`, `QuestionSummary`, `AnswerExcerpt`, `AnswerSummary`, `ReplySummary`, `ReplyCount`, `Topic` (formatted as \"Topic1:40%; Topic2:35%; ...\"), `Sub-Topic` (same format)\n",
    "- **comments_analysis_state.json** (`data/processed/comments_analysis_state.json`): Persistent state file with list of processed (VideoID|CommentID) pairs; updated after each row write\n",
    "- **Console output**: Prints per-video completion messages (video ID, count of processed comments, video title) and final summary (videos submitted, workers used)\n",
    "\n",
    "## Notes / Assumptions\n",
    "\n",
    "- Requires `src.paths` module to define `PROCESSED_DIR`, `RAW_DIR` (see cell 1)\n",
    "- Expects input JSON structure: list of video objects, each with `VideoID`, `Title`, and `Comments` (list of comment dicts with `CommentID`, `Text`, `Author`, `PublishedAt`, optional `ParentID` and `Replies`)\n",
    "- Transcripts CSV must have a column matching `VideoID` (case-insensitive variants like `video_id`, `Id`, `id` are tried) and a column containing \"summary\" (case-insensitive)\n",
    "- GPT is instructed to return valid JSON with exact schema; failure to parse after retries skips the comment\n",
    "- Topic/subtopic breakdowns are normalized to sum to exactly 100% (proportional rounding with drift correction)\n",
    "- Only parent comments (no `ParentID`) are analyzed; replies are included as context in the prompt\n",
    "- CTRL+C triggers graceful shutdown: current tasks finish, then state is saved; re-running resumes from last checkpoint\n",
    "- The cell defines `main()` and runs it via `if __name__ == \"__main__\":` (suitable for both notebook and script execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fee561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import signal\n",
    "import threading\n",
    "import queue\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# =========================\n",
    "# ======== CONFIG =========\n",
    "# =========================\n",
    "# Using portable paths from configuration cell\n",
    "OUTPUT_CSV = COMMENT_ANALYSIS_CSV\n",
    "\n",
    "# Model and API\n",
    "load_dotenv()\n",
    "MODEL_NAME = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")       # set OPENAI_MODEL if needed\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")               # must be set\n",
    "\n",
    "# Concurrency\n",
    "VIDEO_WORKERS = int(os.getenv(\"VIDEO_WORKERS\", \"4\"))       # how many videos to process in parallel\n",
    "\n",
    "# API behavior\n",
    "MAX_RETRIES = 5\n",
    "MIN_DELAY_S = 0.2          # small jitter between calls (optional)\n",
    "MAX_DELAY_S = 0.6\n",
    "\n",
    "# === Token length filter ===\n",
    "# \"skip any comments that are 3 tokens or shorter\" => require at least 4 tokens\n",
    "MIN_PARENT_TOKENS = 4\n",
    "USE_TIKTOKEN = True        # set to False to force whitespace tokenization\n",
    "\n",
    "# Output CSV headers (exact order required)\n",
    "CSV_HEADERS = [\n",
    "    \"VideoID\",\n",
    "    \"CommentID\",\n",
    "    \"QuestionExcerpt\",\n",
    "    \"QuestionSummary\",\n",
    "    \"AnswerExcerpt\",\n",
    "    \"AnswerSummary\",\n",
    "    \"ReplySummary\",\n",
    "    \"ReplyCount\",\n",
    "    \"Topic\",\n",
    "    \"Sub-Topic\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# ====== SIGNAL/STATE =====\n",
    "# =========================\n",
    "stop_requested = False\n",
    "\n",
    "def _sigint_handler(sig, frame):\n",
    "    global stop_requested\n",
    "    stop_requested = True\n",
    "    print(\"\\nCTRL+C detected — finishing current item(s) and saving state...\")\n",
    "\n",
    "signal.signal(signal.SIGINT, _sigint_handler)\n",
    "\n",
    "# =========================\n",
    "# ====== UTILITIES  =======\n",
    "# =========================\n",
    "def ensure_api():\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY is not set. Please export it before running the script.\")\n",
    "\n",
    "def ensure_output_csv(path: Path):\n",
    "    \"\"\"Create the output CSV with header if it doesn't exist.\"\"\"\n",
    "    if not path.exists():\n",
    "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(CSV_HEADERS)\n",
    "\n",
    "def load_existing_pairs_from_output(path: Path) -> set:\n",
    "    \"\"\"Read (VideoID|CommentID) pairs already written to the output CSV.\"\"\"\n",
    "    processed = set()\n",
    "    if path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(path, dtype=str)\n",
    "            if {\"VideoID\", \"CommentID\"}.issubset(df.columns):\n",
    "                for _, row in df[[\"VideoID\", \"CommentID\"]].dropna().iterrows():\n",
    "                    processed.add(f\"{row['VideoID']}|{row['CommentID']}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Could not read existing output CSV to resume: {e}\")\n",
    "    return processed\n",
    "\n",
    "def save_state(state: Dict[str, Any], state_path: Path):\n",
    "    try:\n",
    "        with open(state_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\" Warning: failed to save state file: {e}\")\n",
    "\n",
    "def load_state(state_path: Path) -> Dict[str, Any]:\n",
    "    if state_path.exists():\n",
    "        try:\n",
    "            with open(state_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\" Warning: failed to read state file: {e}\")\n",
    "    return {\"processed_pairs\": []}\n",
    "\n",
    "def find_summary_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Try to find a column whose name contains 'summary' (case-insensitive).\"\"\"\n",
    "    for col in df.columns:\n",
    "        if re.search(r\"summary\", str(col), flags=re.I):\n",
    "            return col\n",
    "    return \"\"\n",
    "\n",
    "def load_transcript_summaries(csv_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Map VideoID -> summary string.\"\"\"\n",
    "    df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "    vid_col = None\n",
    "    for candidate in [\"VideoID\", \"video_id\", \"Id\", \"id\"]:\n",
    "        if candidate in df.columns:\n",
    "            vid_col = candidate\n",
    "            break\n",
    "    if not vid_col:\n",
    "        raise ValueError(\"No VideoID column found in transcripts CSV.\")\n",
    "\n",
    "    summary_col = find_summary_column(df)\n",
    "    if not summary_col:\n",
    "        print(\" No column containing 'summary' found. Proceeding with empty summaries.\")\n",
    "    summaries = {}\n",
    "    for _, row in df.iterrows():\n",
    "        vid = str(row.get(vid_col, \"\")).strip()\n",
    "        if not vid:\n",
    "            continue\n",
    "        summaries[vid] = str(row.get(summary_col, \"\")).strip() if summary_col else \"\"\n",
    "    return summaries\n",
    "\n",
    "def load_comments_json(json_path: Path) -> List[Dict[str, Any]]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def normalize_breakdown(items: List[Dict[str, Any]], key: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Ensure percentages sum to 100. If not, normalize proportionally and round to integers, fixing rounding drift.\n",
    "    key: 'percent' in items\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return items\n",
    "    # Convert to ints\n",
    "    arr = []\n",
    "    for it in items:\n",
    "        try:\n",
    "            pct = int(round(float(it.get(key, 0))))\n",
    "        except Exception:\n",
    "            pct = 0\n",
    "        arr.append({**it, key: pct})\n",
    "    total = sum(x[key] for x in arr)\n",
    "    if total == 100:\n",
    "        return arr\n",
    "    if total <= 0:\n",
    "        # make the first 100, rest 0 as a fallback\n",
    "        if arr:\n",
    "            arr[0][key] = 100\n",
    "            for i in range(1, len(arr)):\n",
    "                arr[i][key] = 0\n",
    "        return arr\n",
    "    # proportional renormalization\n",
    "    factor = 100.0 / total\n",
    "    arr = [{**x, key: int(round(x[key] * factor))} for x in arr]\n",
    "    # fix rounding drift\n",
    "    drift = 100 - sum(x[key] for x in arr)\n",
    "    if drift != 0:\n",
    "        idx = max(range(len(arr)), key=lambda i: arr[i][key])\n",
    "        arr[idx][key] += drift\n",
    "    return arr\n",
    "\n",
    "def format_breakdown_for_csv(items: List[Dict[str, Any]], name_key: str, pct_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert breakdown list to a compact CSV-friendly string:\n",
    "    e.g., \"Rough-in wiring:40%; Branch circuits:35%; Grounding and bonding:25%\"\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for it in items:\n",
    "        label = it.get(name_key, \"\").strip()\n",
    "        pct = it.get(pct_key, 0)\n",
    "        parts.append(f\"{label}:{pct}%\")\n",
    "    return \"; \".join(parts)\n",
    "\n",
    "# === Token length helper ===\n",
    "def token_len(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns approximate token count.\n",
    "    - If tiktoken is available and USE_TIKTOKEN=True, uses cl100k_base.\n",
    "    - Otherwise falls back to a simple whitespace split.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    if USE_TIKTOKEN:\n",
    "        try:\n",
    "            import tiktoken\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(enc.encode(text))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback heuristic based on non-whitespace sequences\n",
    "    return len(re.findall(r\"\\S+\", text))\n",
    "\n",
    "# =========================\n",
    "# ===== OPENAI CALLS  =====\n",
    "# =========================\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert electrical engineering analyst extracting structured data from \"\n",
    "    \"YouTube comment threads. Use ONLY electrical engineering terminology for topics and subtopics.\\n\\n\"\n",
    "    \"Return exactly ONE valid JSON object with this schema:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"QuestionExcerpt\": string,\\n'\n",
    "    '  \"QuestionSummary\": string,\\n'\n",
    "    '  \"ReplySummary\": string,\\n'\n",
    "    '  \"AnswerExcerpt\": string,\\n'\n",
    "    '  \"AnswerSummary\": string,\\n'\n",
    "    '  \"TopicBreakdown\": [{\"topic\": string, \"percent\": integer}, ...],\\n'\n",
    "    '  \"SubtopicBreakdown\": [{\"subtopic\": string, \"percent\": integer}, ...]\\n'\n",
    "    \"}\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Topics and subtopics MUST be electrical engineering terms (e.g., branch circuits, grounding/bonding, NEC 210.52, AFCI/GFCI protection, conductor ampacity, voltage drop, service equipment, panelboards, luminaires, switching, conduit fill, wire gauge, etc.).\\n\"\n",
    "    \"- Include 2–4 items in TopicBreakdown and 2–4 in SubtopicBreakdown; integer percentages that SUM to 100 in each list.\\n\"\n",
    "    '- If the parent comment does not ask a question, set \"QuestionExcerpt\" and \"QuestionSummary\" to \"No Question Present\".\\n'\n",
    "    '- If replies contain a clear solution, include a ≤25-word verbatim \"AnswerExcerpt\" from the best reply and a 1–2 sentence \"AnswerSummary\". Otherwise set both to \"No Solution Present\".\\n'\n",
    "    \"- ReplySummary is 1–2 sentences summarizing the overall replies (agreement, debate, solution, etc.). Use only the provided replies.\\n\"\n",
    "    \"- Prefer precise EE phrasing and NEC references when applicable.\\n\"\n",
    "    \"- Do NOT use markdown or code fences; return a plain JSON object.\"\n",
    ")\n",
    "\n",
    "def build_user_payload(video_summary: str,\n",
    "                       video_title: str,\n",
    "                       parent_comment: Dict[str, Any],\n",
    "                       replies: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Send a compact JSON payload as the user message.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"video_context\": {\n",
    "            \"title\": video_title or \"\",\n",
    "            \"summary\": video_summary or \"\"\n",
    "        },\n",
    "        \"parent_comment\": {\n",
    "            \"id\": parent_comment.get(\"CommentID\", \"\"),\n",
    "            \"author\": parent_comment.get(\"Author\", \"\"),\n",
    "            \"text\": parent_comment.get(\"Text\", \"\"),\n",
    "            \"published_at\": parent_comment.get(\"PublishedAt\", \"\")\n",
    "        },\n",
    "        \"replies\": [\n",
    "            {\n",
    "                \"id\": r.get(\"CommentID\", \"\"),\n",
    "                \"author\": r.get(\"Author\", \"\"),\n",
    "                \"text\": r.get(\"Text\", \"\"),\n",
    "                \"published_at\": r.get(\"PublishedAt\", \"\")\n",
    "            }\n",
    "            for r in (replies or [])\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "def call_gpt(client: OpenAI, user_payload: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call GPT with robust retry/backoff and force JSON response.\n",
    "    \"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_payload},\n",
    "                ],\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            content = resp.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            wait = min(60, 2 ** attempt + random.random() * 0.5)\n",
    "            print(f\" API error: {e} — retrying in {wait:.1f}s (attempt {attempt+1}/{MAX_RETRIES})\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    raise RuntimeError(\"Failed to get a valid response from the model after retries.\")\n",
    "\n",
    "# =========================\n",
    "# === WRITER THREAD LOOP ==\n",
    "# =========================\n",
    "def writer_loop(row_queue: \"queue.Queue\",\n",
    "                output_csv: Path,\n",
    "                state_file: Path,\n",
    "                processed_pairs: set,\n",
    "                processed_pairs_lock: threading.Lock):\n",
    "    \"\"\"\n",
    "    Dedicated writer thread that:\n",
    "      - writes rows to CSV,\n",
    "      - updates processed_pairs,\n",
    "      - persists state after EACH row,\n",
    "      - prints per-video completion after batch.\n",
    "    Queue items:\n",
    "      - dict(type=\"batch\", rows=[row,...], pairs=[pair_key,...], video_id=..., video_title=...)\n",
    "      - None => sentinel to exit\n",
    "    \"\"\"\n",
    "    ensure_output_csv(output_csv)\n",
    "\n",
    "    with open(output_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "\n",
    "        while True:\n",
    "            item = row_queue.get()\n",
    "            if item is None:  # sentinel\n",
    "                row_queue.task_done()\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                if item.get(\"type\") == \"batch\":\n",
    "                    rows: List[List[Any]] = item.get(\"rows\", [])\n",
    "                    pairs: List[str] = item.get(\"pairs\", [])\n",
    "                    video_id = item.get(\"video_id\", \"\")\n",
    "                    video_title = item.get(\"video_title\", \"\")\n",
    "\n",
    "                    written = 0\n",
    "                    for pair_key, row in zip(pairs, rows):\n",
    "                        # double-check for duplicates at write time\n",
    "                        with processed_pairs_lock:\n",
    "                            if pair_key in processed_pairs:\n",
    "                                continue\n",
    "                            writer.writerow(row)\n",
    "                            f_out.flush()\n",
    "                            processed_pairs.add(pair_key)\n",
    "                            # persist state after each row\n",
    "                            state = {\"processed_pairs\": list(processed_pairs)}\n",
    "                            save_state(state, state_file)\n",
    "                            written += 1\n",
    "                    # Per-video completion message after batch actually written\n",
    "                    print(f\" Finished video {video_id} — processed {written} parent comments. ({video_title})\")\n",
    "            finally:\n",
    "                row_queue.task_done()\n",
    "\n",
    "# =========================\n",
    "# ====== VIDEO WORKER =====\n",
    "# =========================\n",
    "def process_one_video(video: Dict[str, Any],\n",
    "                      transcript_summaries: Dict[str, str],\n",
    "                      processed_pairs: set,\n",
    "                      processed_pairs_lock: threading.Lock,\n",
    "                      row_queue: \"queue.Queue\"):\n",
    "    \"\"\"\n",
    "    Process a single video sequentially (parent comments), then push one batch to the writer queue.\n",
    "    \"\"\"\n",
    "    if stop_requested:\n",
    "        return\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    video_id = str(video.get(\"VideoID\", \"\")).strip()\n",
    "    video_title = str(video.get(\"Title\", \"\") or \"\").strip()\n",
    "    summary = transcript_summaries.get(video_id, \"\")\n",
    "\n",
    "    comments = video.get(\"Comments\", []) or []\n",
    "    # parent comments only and >= MIN_PARENT_TOKENS tokens\n",
    "    parent_comments = [\n",
    "        c for c in comments\n",
    "        if not c.get(\"ParentID\") and token_len(str(c.get(\"Text\", \"\"))) >= MIN_PARENT_TOKENS\n",
    "    ]\n",
    "\n",
    "    rows_to_write: List[List[Any]] = []\n",
    "    pairs_to_write: List[str] = []\n",
    "\n",
    "    for parent in parent_comments:\n",
    "        if stop_requested:\n",
    "            break\n",
    "\n",
    "        comment_id = str(parent.get(\"CommentID\", \"\")).strip()\n",
    "        pair_key = f\"{video_id}|{comment_id}\"\n",
    "\n",
    "        # Skip if already processed (thread-safe read)\n",
    "        with processed_pairs_lock:\n",
    "            if pair_key in processed_pairs:\n",
    "                continue\n",
    "\n",
    "        replies = parent.get(\"Replies\", []) or []\n",
    "        reply_count = len(replies)\n",
    "\n",
    "        # Build payload and call GPT\n",
    "        payload = build_user_payload(summary, video_title, parent, replies)\n",
    "        try:\n",
    "            result = call_gpt(client, payload)\n",
    "        except Exception as e:\n",
    "            print(f\" Skipping comment {comment_id} on video {video_id} due to repeated API errors: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Normalize breakdowns\n",
    "        topics = normalize_breakdown(result.get(\"TopicBreakdown\", []), \"percent\")\n",
    "        subs = normalize_breakdown(result.get(\"SubtopicBreakdown\", []), \"percent\")\n",
    "\n",
    "        # Prepare CSV row\n",
    "        question_excerpt = str(result.get(\"QuestionExcerpt\", \"\")).strip()\n",
    "        question_summary = str(result.get(\"QuestionSummary\", \"\")).strip()\n",
    "        reply_summary = str(result.get(\"ReplySummary\", \"\")).strip()\n",
    "\n",
    "        answer_excerpt = str(result.get(\"AnswerExcerpt\", \"\")).strip() or \"No Solution Present\"\n",
    "        answer_summary = str(result.get(\"AnswerSummary\", \"\")).strip() or \"No Solution Present\"\n",
    "\n",
    "        topic_str = format_breakdown_for_csv(topics, \"topic\", \"percent\")\n",
    "        subtopic_str = format_breakdown_for_csv(subs, \"subtopic\", \"percent\")\n",
    "\n",
    "        row = [\n",
    "            video_id,\n",
    "            comment_id,\n",
    "            question_excerpt,\n",
    "            question_summary,\n",
    "            answer_excerpt,\n",
    "            answer_summary,\n",
    "            reply_summary,\n",
    "            reply_count,\n",
    "            topic_str,\n",
    "            subtopic_str,\n",
    "        ]\n",
    "        rows_to_write.append(row)\n",
    "        pairs_to_write.append(pair_key)\n",
    "\n",
    "        # tiny jitter helps avoid rate limits\n",
    "        time.sleep(random.uniform(MIN_DELAY_S, MAX_DELAY_S))\n",
    "\n",
    "    # Send one batch to writer (writer will print the per-video completion after writing)\n",
    "    if rows_to_write:\n",
    "        row_queue.put({\n",
    "            \"type\": \"batch\",\n",
    "            \"rows\": rows_to_write,\n",
    "            \"pairs\": pairs_to_write,\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title\n",
    "        })\n",
    "    else:\n",
    "        # Still print a completion message for transparency (0 processed)\n",
    "        row_queue.put({\n",
    "            \"type\": \"batch\",\n",
    "            \"rows\": [],\n",
    "            \"pairs\": [],\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title\n",
    "        })\n",
    "\n",
    "# =========================\n",
    "# ====== MAIN LOGIC  ======\n",
    "# =========================\n",
    "def main():\n",
    "    ensure_api()\n",
    "\n",
    "    # Prep output and state\n",
    "    ensure_output_csv(OUTPUT_CSV)\n",
    "    processed_pairs = load_existing_pairs_from_output(OUTPUT_CSV)\n",
    "    state = load_state(STATE_FILE)\n",
    "    # Merge state pairs (if any) into processed set\n",
    "    for p in state.get(\"processed_pairs\", []):\n",
    "        processed_pairs.add(p)\n",
    "\n",
    "    # Lock to protect processed_pairs across threads\n",
    "    processed_pairs_lock = threading.Lock()\n",
    "\n",
    "    # Load inputs (use comments with transcripts if available, else raw comments)\n",
    "    if COMMENTS_WITH_TRANSCRIPTS_JSON.exists():\n",
    "        comments_by_video = load_comments_json(COMMENTS_WITH_TRANSCRIPTS_JSON)\n",
    "    else:\n",
    "        comments_by_video = load_comments_json(COMMENTS_JSON)\n",
    "    transcript_summaries = load_transcript_summaries(TRANSCRIPTS_CSV)\n",
    "\n",
    "    total_videos = len(comments_by_video)\n",
    "\n",
    "    # Start the writer thread\n",
    "    row_queue: \"queue.Queue\" = queue.Queue()\n",
    "    writer_thread = threading.Thread(\n",
    "        target=writer_loop,\n",
    "        args=(row_queue, OUTPUT_CSV, STATE_FILE, processed_pairs, processed_pairs_lock),\n",
    "        daemon=True\n",
    "    )\n",
    "    writer_thread.start()\n",
    "\n",
    "    # Submit video tasks\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=VIDEO_WORKERS) as executor:\n",
    "        for video in comments_by_video:\n",
    "            if stop_requested:\n",
    "                break\n",
    "            futures.append(executor.submit(\n",
    "                process_one_video,\n",
    "                video,\n",
    "                transcript_summaries,\n",
    "                processed_pairs,\n",
    "                processed_pairs_lock,\n",
    "                row_queue\n",
    "            ))\n",
    "\n",
    "        # Wait for all submitted futures to finish\n",
    "        try:\n",
    "            for _ in as_completed(futures):\n",
    "                if stop_requested:\n",
    "                    # We still wait for running tasks to finish their current comment\n",
    "                    pass\n",
    "        except KeyboardInterrupt:\n",
    "            # Redundant safety; SIGINT is already handled\n",
    "            pass\n",
    "\n",
    "    # Ensure all queued rows are written\n",
    "    row_queue.join()\n",
    "    # Stop writer thread\n",
    "    row_queue.put(None)\n",
    "    writer_thread.join()\n",
    "\n",
    "    if stop_requested:\n",
    "        print(\"Progress saved. Resume by running the script again.\")\n",
    "    else:\n",
    "        print(f\"All done. Submitted {len(futures)}/{total_videos} videos with {VIDEO_WORKERS} workers.\")\n",
    "\n",
    "# =========================\n",
    "# ========= RUN ===========\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        # Redundant safety — we already trap SIGINT\n",
    "        print(\"\\n Interrupted. Progress saved where possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63946f01",
   "metadata": {},
   "source": [
    "### 02_Comment Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here visualizing the comment analysis results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
